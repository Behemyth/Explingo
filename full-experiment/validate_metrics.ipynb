{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:25:49.351679Z",
     "start_time": "2024-09-13T18:25:48.402167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import metrics\n",
    "import dspy\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import random\n",
    "import examples\n",
    "\n",
    "with open(os.path.join(\"..\", \"keys.yaml\"), \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "    \n",
    "grader = dspy.OpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            model_type=\"chat\",\n",
    "            max_tokens=500,\n",
    "            api_key=openai_api_key,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "    \n",
    "os.environ[\"DSP_CACHEBOOL\"] = \"False\"\n",
    "\n",
    "max_score = metrics.MAX_SCORE"
   ],
   "id": "d5efeb7510b98046",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:25:49.367462Z",
     "start_time": "2024-09-13T18:25:49.352681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "empty_input = dspy.Example()"
   ],
   "id": "d57e310a4b23ed8f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test 1: Conciseness"
   ],
   "id": "a98751c87bdae1f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:25:49.382475Z",
     "start_time": "2024-09-13T18:25:49.369508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "one_feature_input = dspy.Example(explanation='(word, .05, .05)')\n",
    "two_feature_input = dspy.Example(explanation='(word, .05, .05), (word, .05, .05)')\n",
    "\n",
    "short_output = dspy.Prediction(narrative='word ')\n",
    "short_result = metrics.conciseness(one_feature_input, short_output, max_optimal_length_per_feature=10)\n",
    "assert short_result == max_score, short_result\n",
    "\n",
    "long_output = dspy.Prediction(narrative='word ' * 21)\n",
    "long_result = metrics.conciseness(one_feature_input, long_output, max_optimal_length_per_feature=10)\n",
    "assert long_result == 0, long_result\n",
    "\n",
    "med_output = dspy.Prediction(narrative='word ' * 15)\n",
    "med_result = metrics.conciseness(one_feature_input, med_output, max_optimal_length_per_feature=10)\n",
    "assert med_result == max_score / 2, med_result\n",
    "\n",
    "two_word_output = dspy.Prediction(narrative='word ' * 18)\n",
    "two_word_result = metrics.conciseness(two_feature_input, two_word_output, max_optimal_length_per_feature=10)\n",
    "assert two_word_result == max_score, two_word_result\n",
    "\n",
    "two_word_output = dspy.Prediction(narrative='word ' * 30)\n",
    "two_word_result = metrics.conciseness(two_feature_input, two_word_output, max_optimal_length_per_feature=10)\n",
    "assert two_word_result == max_score / 2, two_word_result"
   ],
   "id": "73a06bbd8dab9e9e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test 2: Accuracy"
   ],
   "id": "57d5cb5147ddac36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:25:55.404362Z",
     "start_time": "2024-09-13T18:25:49.383453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_input = dspy.Example(explanation='(Size, 5, 10), (Color, Red, -8)', \n",
    "                          explanation_format='(feature_name, feature_value, SHAP contribution)')\n",
    "\n",
    "accurate_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10. The color being red decreased the prediction by 8.')\n",
    "accurate_result = metrics.accuracy(test_input, accurate_output, grader=grader)\n",
    "assert accurate_result == max_score, accurate_result\n",
    "\n",
    "inaccurate_output = dspy.Prediction(narrative='The size being 4 increased the prediction by 11. The color being red decreased the prediction by 9.')\n",
    "inaccurate_result = metrics.accuracy(test_input, inaccurate_output, grader=grader)\n",
    "assert inaccurate_result == 0, inaccurate_result\n",
    "\n",
    "incomplete_but_accurate_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10.')\n",
    "incomplete_but_accurate_result = metrics.accuracy(test_input, incomplete_but_accurate_output, grader=grader)\n",
    "assert incomplete_but_accurate_result == max_score, incomplete_but_accurate_result"
   ],
   "id": "5325ca464e28753e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:30:52.627572Z",
     "start_time": "2024-09-13T18:30:35.702116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy_dataset = json.load(open(\"accuracy_tests.json\"))\n",
    "\n",
    "preds = []\n",
    "actuals = []\n",
    "for example in accuracy_dataset:\n",
    "    test_input = dspy.Example(explanation=example[\"explanation\"], explanation_format=example[\"explanation_format\"])\n",
    "    test_output = dspy.Prediction(narrative=example[\"narrative\"])\n",
    "    result = metrics.accuracy(test_input, test_output, grader)\n",
    "    preds.append(result)\n",
    "    actuals.append(example[\"accuracy_score\"])\n",
    "    if result != example[\"accuracy_score\"]:\n",
    "        print(f'Explanation: {example[\"explanation\"]}')\n",
    "        print(f'Output: {example[\"narrative\"]}')\n",
    "        print(f'Expected: {example[\"accuracy_score\"]}. Got: {result}')\n",
    "\n",
    "cm = confusion_matrix(actuals, preds, labels=[0.0, 4.0])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0.0, 4.0])\n",
    "disp.plot()\n",
    "\n",
    "print(f\"Accuracy: {sum([1 for p, a in zip(preds, actuals) if p == a]) / len(preds)}\")\n"
   ],
   "id": "584dbe55909d40a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: (Evaluates the quality of the material on the exterior, TA, -1868.01), (Exterior covering on house, VinylSd, 16798.14)\n",
      "Output: The average quality of exterior material and  the vinyl siding decrease the price prediction by $1,868.01 and $16,798.14, respectively.\n",
      "Expected: 0. Got: 4.0\n",
      "Explanation: (odor, none, 0.14), (gill-size, broad, 0.07), (spore-print-color, brown, 0.05)\n",
      "Output: The narrow gill size increases the probability of the mushroom being poisonous by 0.14. The broad gill size increases the probability by 0.07, and the brown spore print color increases the probability by 0.05.\n",
      "Expected: 4. Got: 0.0\n",
      "Accuracy: 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "grader.inspect_history(n=1)",
   "id": "906af667eb9db312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test 3: Completeness",
   "id": "fdcc3357a2ace626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_input = dspy.Example(explanation='(Size, 5, 10), (Color, Red, -8), (Shape, Square, 3)', \n",
    "                          explanation_format='(feature_name, feature_value, SHAP contribution)')\n",
    "\n",
    "complete_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10. The color being red decreased the prediction by 8. The shape being square increased the prediction by 3.')\n",
    "complete_result = metrics.completeness(test_input, complete_output, grader)\n",
    "assert complete_result == max_score, complete_result\n",
    "\n",
    "incomplete_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10')\n",
    "incomplete_result = metrics.completeness(test_input, incomplete_output, grader)\n",
    "assert incomplete_result == 0, incomplete_result"
   ],
   "id": "e378c2e99b45c849",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "completeness_dataset = json.load(open(\"completeness_tests.json\"))\n",
    "\n",
    "preds = []\n",
    "actuals = []\n",
    "rationalization_when_failing = defaultdict(int)\n",
    "for example in completeness_dataset:\n",
    "    test_input = dspy.Example(explanation=example[\"explanation\"], explanation_format=example[\"explanation_format\"])\n",
    "    test_output = dspy.Prediction(narrative=example[\"narrative\"])\n",
    "    result = metrics.completeness(test_input, test_output, grader)\n",
    "    preds.append(result)\n",
    "    actuals.append(example[\"completeness_score\"])\n",
    "    if result != example[\"completeness_score\"]:\n",
    "        print(f'Explanation: {example[\"explanation\"]}')\n",
    "        print(f'Output: {example[\"narrative\"]}')\n",
    "        print(f'Expected: {example[\"completeness_score\"]}. Got: {result}')\n",
    "\n",
    "cm = confusion_matrix(actuals, preds, labels=[0.0, 2.0, 4.0])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0.0, 2.0, 4.0])\n",
    "disp.plot()\n",
    "\n",
    "# print accuracy\n",
    "print(f\"Accuracy: {sum([1 for p, a in zip(preds, actuals) if p == a]) / len(preds)}\")"
   ],
   "id": "bcd52acb9a3eb86e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "grader.inspect_history(n=1)",
   "id": "e0949c80a603c982",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test 4: Fluency\n",
    "\n",
    "We test fluency with two methods:\n",
    "1. A few basic unit tests\n",
    "2. Ensuring that fluency consistently scores higher on whatever dataset is labeled gold standard dataset than on other datasets"
   ],
   "id": "150426ee379082ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "good_narratives = ['The large size increased the prediction by 100, while the green color decreased it by about 20',\n",
    "                   \"The item's small size of 2 decreased the prediction by about 4, while its blue color increased it by 3\"]\n",
    "\n",
    "fluent_output = dspy.Prediction(narrative='The large size increased the prediction by about 10, while the red color decreased it by 8.')\n",
    "fluent_result = metrics.fluency(empty_input, fluent_output, grader, good_narratives=good_narratives)\n",
    "\n",
    "influent_output = dspy.Prediction(narrative='The size (5) increased the prediction (10.5). The color (red) decreased it (8.2)')\n",
    "influent_result = metrics.fluency(empty_input, influent_output, grader, good_narratives=good_narratives)\n",
    "\n",
    "assert influent_result < fluent_result, str(influent_result) + \" \" + str(fluent_result)\n",
    "assert fluent_result == max_score, fluent_result\n",
    "\n",
    "very_influent_output = dspy.Prediction(narrative='Size 5 10.5 color red')\n",
    "very_influent_result = metrics.fluency(empty_input, very_influent_output, grader, good_narratives=good_narratives)\n",
    "assert very_influent_result == 0, very_influent_result"
   ],
   "id": "e79811ad7e430624",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "validation_datasets = [f for f in os.listdir(\"eval_data\") if os.path.isfile(os.path.join(\"eval_data\", f))]\n",
    "\n",
    "loaded_datasets = {}\n",
    "for dataset in validation_datasets:\n",
    "    print(f\"Loading dataset: {dataset}\")\n",
    "    labeled_train, _, _, _ = examples.get_data(os.path.join(\"eval_data\", dataset), split=1)\n",
    "    loaded_datasets[dataset] = labeled_train\n",
    "\n",
    "# IDENTIFY BEST NUMBER OF EXAMPLES\n",
    "#results = []\n",
    "with open(\"fluency_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "def validate_fluency(gold_standard_dataset, N):\n",
    "    example_good_narratives = random.sample([d.narrative for d in loaded_datasets[gold_standard_dataset]], N)\n",
    "    all_results = {}\n",
    "    for dataset in loaded_datasets:\n",
    "        all_results[dataset] = 0\n",
    "        for example in loaded_datasets[dataset]:\n",
    "            all_results[dataset] += metrics.fluency(empty_input, example, grader, good_narratives=example_good_narratives)\n",
    "        all_results[dataset] /= len(loaded_datasets[dataset])\n",
    "    print(f\"Gold standard: {gold_standard_dataset}\")\n",
    "    results.append({\"gold_standard\": gold_standard_dataset, \"N\": N, \"results\": all_results})\n",
    "    # save results as json\n",
    "    with open(\"fluency_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "        print(f\"Results saved. Gold standard: {gold_standard_dataset}, N: {N}\")\n",
    "    \n",
    "    for dataset in all_results:\n",
    "        if dataset == gold_standard_dataset:\n",
    "            print(f\"**Dataset: {dataset}, Average score: {all_results[dataset]:.2f}\")\n",
    "        else:\n",
    "            print(f\"--Dataset: {dataset}, Average score: {all_results[dataset]:.2f}\")\n",
    "    print()\n",
    "    \n",
    "for dataset in loaded_datasets:\n",
    "    for N in [1, 3, 5, 7]:\n",
    "        validate_fluency(dataset, N)"
   ],
   "id": "42636ee1b6f038ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "grader.inspect_history(n=1)",
   "id": "5f701c7fdfa9f7eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test 5: Context-Awareness",
   "id": "f45a6434ddf1077e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_input = dspy.Example(explanation='(Size, 5, 10), (Color, Red, -8)', explanation_format='(feature_name, feature_value, SHAP contribution)')\n",
    "\n",
    "context_aware_output = dspy.Prediction(narrative = 'The large size of 5 increased the price by 10. The red color decreased the price by 8.',\n",
    "                                       rationalization='Large items fit more contents, making them desirable, so they increase the price. People associate red with the devil, so this color decreases the price.')\n",
    "context_aware_result = metrics.context_awareness(test_input, context_aware_output, grader)\n",
    "assert context_aware_result >= max_score/2, context_aware_result\n",
    "\n",
    "context_unaware_output = dspy.Prediction(narrative = 'The large size of 5 increased the price by 10. The red color decreased the price by 8.', \n",
    "                                         rationalization='No idea why.')\n",
    "context_unaware_result = metrics.context_awareness(test_input, context_unaware_output, grader)\n",
    "assert context_unaware_result == 0, context_unaware_result"
   ],
   "id": "8375bcc0043ea6ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Compare Datasets\n",
    "\n",
    "In the following code block, we verify our metric functionality by comparing average score on our gold standard dataset (used to tune the metrics) to other datasets that use different styles of explanations. We expect the gold standard average score to be very close to 2*len(metrics) (since each metric is scored on a scale of 0-2), and the other datasets to be lower.\n",
    " \n",
    "TODO: We do not currently verify the context awareness metric, as the gold-standard dataset does not include a rationalization"
   ],
   "id": "ff5cda333b3fc86f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metric_verification_datasets = [\"gold_standards.json\", \"unaligned_examples_1.json\", \"unaligned_examples_2.json\"]\n",
    "\n",
    "labeled_train, labeled_eval, unlabeled_train, unlabeled_eval = examples.get_data(\"gold_standards.json\")\n",
    "train_data = labeled_train + unlabeled_train\n",
    "eval_data = labeled_eval + unlabeled_eval\n",
    "max_optimal_length = max([len(d.narrative) for d in labeled_train])\n",
    "\n",
    "example_good_narratives = random.sample([d.narrative for d in labeled_train], 5)\n",
    "example_bad_narratives = random.sample([d.bad_narrative for d in labeled_train if hasattr(d, \"bad_narrative\")], 5)\n",
    "\n",
    "# Example datasets do not include a rationalization, so we skip context awareness \n",
    "ver_metrics = metrics.Metrics(\n",
    "    [\n",
    "        metrics.accuracy,\n",
    "        metrics.fluency,\n",
    "        metrics.conciseness,\n",
    "        #metrics.completeness\n",
    "    ], verbose=0, openai_key=openai_api_key,\n",
    "    metric_kwargs={\"conciseness\": {\"max_optimal_length\": max_optimal_length},\n",
    "                   \"fluency\": {\"good_narratives\": example_good_narratives, \"bad_narratives\": example_bad_narratives}}\n",
    ")\n",
    "\n",
    "for dataset in metric_verification_datasets: \n",
    "    labeled_train, _, _, _ = examples.get_data(dataset, split=1)\n",
    "    all_results = None\n",
    "    score = 0\n",
    "    for example in labeled_train:\n",
    "        result = ver_metrics(example, example)\n",
    "        score += result[0]\n",
    "        if all_results is None:\n",
    "            all_results = result[1]\n",
    "        else:\n",
    "            all_results += result[1]\n",
    "    print(f\"Dataset: {dataset}, Average score: {score/len(labeled_train)}/{len(ver_metrics.metric_funcs)*4.0}\")\n",
    "    print(all_results/len(labeled_train))"
   ],
   "id": "7b59478832886fed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Failing Cases"
   ],
   "id": "cc5f59cef2f2ca2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inaccurate_output = dspy.Prediction(narrative='The size being 5 decreased the prediction by 10. The color being red decreased the prediction by 8.')\n",
    "inaccurate_result = metrics.accuracy(test_input, inaccurate_output, grader=grader)\n",
    "assert inaccurate_result == 0, inaccurate_result\n",
    "\n",
    "misleading_output = dspy.Prediction(narrative='Surprisingly, the large house size of 5 increased the predicted price')\n",
    "misleading_result = metrics.accuracy(test_input, misleading_output, grader=grader)\n",
    "assert 0 < misleading_result < 4, misleading_result"
   ],
   "id": "761fe0aa9fc2f680",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Futher analysis for fluency",
   "id": "db7e7d9cb70ee9c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"fluency_results.json\", \"r\") as f:\n",
    "    fluency_results = json.load(f)"
   ],
   "id": "582e13ac7dbaa466",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clean_results = []\n",
    "for item in fluency_results:\n",
    "    for dataset in item[\"results\"]:\n",
    "        clean_results.append({\"gold_standard\": item[\"gold_standard\"], \"N\": item[\"N\"], \"dataset\": dataset, \"score\": item[\"results\"][dataset]})\n",
    "df = pd.DataFrame(clean_results)\n",
    "\n",
    "df[\"on_gold\"] = df[\"gold_standard\"] == df[\"dataset\"]\n",
    "df[\"Type\"] = df[\"on_gold\"].apply(lambda x: \"Exemplar\" if x else \"Other\")\n",
    "df_full = df.drop(columns=[\"gold_standard\", \"on_gold\"])"
   ],
   "id": "5c9ab560813b91c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df_full.groupby([\"N\", \"dataset\", \"Type\"]).mean().reset_index()\n",
    "df"
   ],
   "id": "7e6e4ee34fb1fc90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set figure size\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "sns.boxplot(data=df, x=\"N\", y=\"score\", hue=\"Type\", dodge=False, palette=\"deep\")\n",
    "#sns.swarmplot(data=df, x=\"N\", y=\"score\", hue=\"Type\", dodge=False, palette=\"dark\")\n",
    "\n",
    "plt.ylabel(\"Fluency Score\")\n",
    "plt.xlabel(\"Number of Exemplars\")\n",
    "plt.legend(title=\"Narrative Dataset\", loc=\"lower left\", bbox_to_anchor=(-.08, -.45))\n",
    "plt.savefig(\"fluency_results.png\", bbox_inches=\"tight\")"
   ],
   "id": "3861ca738490d4c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute KL divergence between df[\"Dataset\"] == \"Exemplar\" and df[\"Dataset\"] == \"Other\"\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    p, q = np.array(p), np.array(q)\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "for N in [1, 3, 5]:\n",
    "    df_N = df[(df[\"N\"] == N)]\n",
    "    exemplar_scores = df_N[df_N[\"Type\"] == \"Exemplar\"]\n",
    "    other_scores = df_N[df_N[\"Type\"] == \"Other\"]\n",
    "    average_difference = abs(exemplar_scores[\"score\"].mean() - other_scores[\"score\"].mean())\n",
    "    difference_per_dataset = {}\n",
    "    for dataset in df_N[\"dataset\"].unique():\n",
    "        exemplar_dataset = exemplar_scores[exemplar_scores[\"dataset\"] == dataset][\"score\"].mean()\n",
    "        other_dataset = other_scores[other_scores[\"dataset\"] == dataset][\"score\"].mean()\n",
    "        difference_per_dataset[dataset] = abs(exemplar_dataset - other_dataset)\n",
    "    min_difference_per_dataset = np.min(list(difference_per_dataset.values()))\n",
    "    kl_divergence_score = kl_divergence(exemplar_scores[\"score\"], other_scores[\"score\"])\n",
    "    percent_difference = average_difference / exemplar_scores[\"score\"].mean() * 100\n",
    "    \n",
    "    \n",
    "    print(f\"N: {N}, Average difference: {average_difference:.2f}\")\n",
    "    print(f\"N: {N}, Min difference per dataset: {min_difference_per_dataset:.2f}\")\n",
    "    print(f\"N: {N}, KL Divergence: {kl_divergence_score:.2f}\")\n",
    "    print(f\"N: {N}, Percent difference: {percent_difference:.2f}\")\n",
    "    print(\"---\")\n",
    "    "
   ],
   "id": "90fc8d9a14b28bfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_full",
   "id": "382e95641d40a45a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d9bb95538361f013",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
