{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-23T12:42:32.986661Z",
     "start_time": "2024-08-23T12:42:31.755895Z"
    }
   },
   "source": [
    "import metrics2 as metrics\n",
    "import dspy\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "with open(os.path.join(\"..\", \"keys.yaml\"), \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "    \n",
    "grader = dspy.OpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            model_type=\"chat\",\n",
    "            max_tokens=2000,\n",
    "            api_key=openai_api_key,\n",
    "        )\n",
    "    \n",
    "max_score = metrics.MAX_SCORE"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T12:42:32.989865Z",
     "start_time": "2024-08-23T12:42:32.987667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "empty_input = dspy.Example()"
   ],
   "id": "a4319ae8323cd4de",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test 1: Conciseness"
   ],
   "id": "403a161d8d30b1cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T12:42:32.997684Z",
     "start_time": "2024-08-23T12:42:32.989865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "short_output = dspy.Prediction(narrative='word ')\n",
    "short_result = metrics.conciseness(empty_input, short_output, max_optimal_length=10)\n",
    "assert short_result == max_score, short_result\n",
    "\n",
    "long_output = dspy.Prediction(narrative='word ' * 21)\n",
    "long_result = metrics.conciseness(empty_input, long_output, max_optimal_length=10)\n",
    "assert long_result == 0, long_result\n",
    "\n",
    "med_output = dspy.Prediction(narrative='word ' * 15)\n",
    "med_result = metrics.conciseness(empty_input, med_output, max_optimal_length=10)\n",
    "assert med_result == max_score / 2, med_result"
   ],
   "id": "9cb6db83bfc4843e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test 2: Accuracy"
   ],
   "id": "79045feb8d8af121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T12:42:33.876158Z",
     "start_time": "2024-08-23T12:42:32.997684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_input = dspy.Example(explanation='(Size, 5, 10), (Color, Red, -8)', \n",
    "                          explanation_format='(feature_name, feature_value, SHAP contribution)')\n",
    "\n",
    "accurate_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10. The color being red decreased the prediction by 8.')\n",
    "accurate_result = metrics.accuracy(test_input, accurate_output, grader=grader)\n",
    "assert accurate_result == max_score, accurate_result\n",
    "\n",
    "inaccurate_output = dspy.Prediction(narrative='The size being 4 increased the prediction by 11. The color being red decreased the prediction by 9.')\n",
    "inaccurate_result = metrics.accuracy(test_input, inaccurate_output, grader=grader)\n",
    "assert inaccurate_result == 0, inaccurate_result\n",
    "\n",
    "incomplete_but_accurate_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10.')\n",
    "incomplete_but_accurate_result = metrics.accuracy(test_input, incomplete_but_accurate_output, grader=grader)\n",
    "assert incomplete_but_accurate_result == max_score, incomplete_but_accurate_result"
   ],
   "id": "14168fb360045bcc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test 3: Completeness"
   ],
   "id": "583c04d26f9b3e7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T12:42:34.547199Z",
     "start_time": "2024-08-23T12:42:33.877164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_input = dspy.Example(explanation='(Size, 5, 10), (Color, Red, -8), (Shape, Square, 3)', \n",
    "                          explanation_format='(feature_name, feature_value, SHAP contribution)')\n",
    "\n",
    "complete_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10. The color being red decreased the prediction by 8. The shape being square increased the prediction by 3.')\n",
    "complete_result = metrics.completeness(test_input, complete_output, grader)\n",
    "assert complete_result == max_score, complete_result\n",
    "\n",
    "incomplete_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10. The color being red decreased the prediction by 8.')\n",
    "incomplete_result = metrics.completeness(test_input, incomplete_output, grader)\n",
    "assert incomplete_result == 0, incomplete_result"
   ],
   "id": "23379cf2fdd4aa59",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test 4: Fluency"
   ],
   "id": "f513c89283bc9ed5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T12:42:35.435246Z",
     "start_time": "2024-08-23T12:42:34.547199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "good_narratives = ['The large size increased the prediction by 100, while the green color decreased it by about 20',\n",
    "                   \"The item's small size of 2 decreased the prediction by about 4, while its blue color increased it by 3\"]\n",
    "\n",
    "fluent_output = dspy.Prediction(narrative='The large size of the item increased the prediction by about 10, while the red color decreased it by 8.')\n",
    "fluent_result = metrics.fluency(empty_input, fluent_output, grader, good_narratives=good_narratives)\n",
    "\n",
    "influent_output = dspy.Prediction(narrative='The size being 5 increased the prediction by 10.5. The color being red decreased it by 8.2')\n",
    "influent_result = metrics.fluency(empty_input, influent_output, grader, good_narratives=good_narratives)\n",
    "\n",
    "assert influent_result < fluent_result\n",
    "assert fluent_result == max_score, fluent_result\n",
    "\n",
    "very_influent_output = dspy.Prediction(narrative='Size 5 10.5 color red')\n",
    "very_influent_result = metrics.fluency(empty_input, very_influent_output, grader, good_narratives=good_narratives)\n",
    "assert very_influent_result == 0, very_influent_result"
   ],
   "id": "2214a9f493150063",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test 5: Context-Awareness"
   ],
   "id": "c3bfd9e3b5c0bbfd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T12:42:35.966558Z",
     "start_time": "2024-08-23T12:42:35.436251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_input = dspy.Example(explanation='(Size, 5, 10), (Color, Red, -8)', explanation_format='(feature_name, feature_value, SHAP contribution)')\n",
    "\n",
    "context_aware_output = dspy.Prediction(narrative = 'The large size of 5 increased the price by 10. The red color decreased the price by 8.',\n",
    "                                       rationalization='Large items fit more contents, making them desirable, so they increase the price. People associate red with the devil, so this color decreases the price.')\n",
    "context_aware_result = metrics.context_awareness(test_input, context_aware_output, grader)\n",
    "assert context_aware_result >= max_score/2, context_aware_result\n",
    "\n",
    "context_unaware_output = dspy.Prediction(narrative = 'The large size of 5 increased the price by 10. The red color decreased the price by 8.', \n",
    "                                         rationalization='No idea why.')\n",
    "context_unaware_result = metrics.context_awareness(test_input, context_unaware_output, grader)\n",
    "assert context_unaware_result == 0, context_unaware_result"
   ],
   "id": "8375bcc0043ea6ee",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare Datasets\n",
    "\n",
    "In the following code block, we verify our metric functionality by comparing average score on our gold standard dataset (used to tune the metrics) to other datasets that use different styles of explanations. We expect the gold standard average score to be very close to 2*len(metrics) (since each metric is scored on a scale of 0-2), and the other datasets to be lower.\n",
    " \n",
    "TODO: We do not currently verify the context awareness metric, as the gold-standard dataset does not include a rationalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff5cda333b3fc86f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: gold_standards.json, Average score: 11.6/12.0\n",
      "accuracy       4.0\n",
      "fluency        3.6\n",
      "conciseness    4.0\n",
      "dtype: float64\n",
      "Dataset: unaligned_examples_1.json, Average score: 11.125/12.0\n",
      "accuracy       4.000\n",
      "fluency        3.125\n",
      "conciseness    4.000\n",
      "dtype: float64\n",
      "Dataset: unaligned_examples_2.json, Average score: 11.0/12.0\n",
      "accuracy       4.0\n",
      "fluency        3.0\n",
      "conciseness    4.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import examples\n",
    "\n",
    "metric_verification_datasets = [\"gold_standards.json\", \"unaligned_examples_1.json\", \"unaligned_examples_2.json\"]\n",
    "\n",
    "labeled_train, labeled_eval, unlabeled_train, unlabeled_eval = examples.get_data(\"gold_standards.json\")\n",
    "train_data = labeled_train + unlabeled_train\n",
    "eval_data = labeled_eval + unlabeled_eval\n",
    "max_optimal_length = max([len(d.narrative) for d in labeled_train])\n",
    "\n",
    "example_good_narratives = random.sample([d.narrative for d in labeled_train], 5)\n",
    "example_bad_narratives = random.sample([d.bad_narrative for d in labeled_train if hasattr(d, \"bad_narrative\")], 5)\n",
    "\n",
    "# Example datasets do not include a rationalization, so we skip context awareness \n",
    "ver_metrics = metrics.Metrics(\n",
    "    [\n",
    "        metrics.accuracy,\n",
    "        metrics.fluency,\n",
    "        metrics.conciseness,\n",
    "        #metrics.completeness\n",
    "    ], verbose=0, openai_key=openai_api_key,\n",
    "    metric_kwargs={\"conciseness\": {\"max_optimal_length\": max_optimal_length},\n",
    "                   \"fluency\": {\"good_narratives\": example_good_narratives, \"bad_narratives\": example_bad_narratives}}\n",
    ")\n",
    "\n",
    "for dataset in metric_verification_datasets: \n",
    "    labeled_train, _, _, _ = examples.get_data(dataset, split=1)\n",
    "    all_results = None\n",
    "    score = 0\n",
    "    for example in labeled_train:\n",
    "        result = ver_metrics(example, example)\n",
    "        score += result[0]\n",
    "        if all_results is None:\n",
    "            all_results = result[1]\n",
    "        else:\n",
    "            all_results += result[1]\n",
    "    print(f\"Dataset: {dataset}, Average score: {score/len(labeled_train)}/{len(ver_metrics.metric_funcs)*4.0}\")\n",
    "    print(all_results/len(labeled_train))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-23T12:43:23.114362Z",
     "start_time": "2024-08-23T12:42:35.966558Z"
    }
   },
   "id": "7b59478832886fed",
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Failing Cases"
   ],
   "id": "cc5f59cef2f2ca2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T12:43:24.136123Z",
     "start_time": "2024-08-23T12:43:23.114362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inaccurate_output = dspy.Prediction(narrative='The size being 5 decreased the prediction by 10. The color being red decreased the prediction by 8.')\n",
    "inaccurate_result = metrics.accuracy(test_input, inaccurate_output, grader=grader)\n",
    "assert inaccurate_result == 0, inaccurate_result\n",
    "\n",
    "misleading_output = dspy.Prediction(narrative='Surprisingly, the large house size of 5 increased the predicted price')\n",
    "misleading_result = metrics.accuracy(test_input, misleading_output, grader=grader)\n",
    "assert 0 < misleading_result < 4, misleading_result"
   ],
   "id": "761fe0aa9fc2f680",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "0.0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m misleading_output \u001B[38;5;241m=\u001B[39m dspy\u001B[38;5;241m.\u001B[39mPrediction(narrative\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSurprisingly, the large house size of 5 increased the predicted price\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m misleading_result \u001B[38;5;241m=\u001B[39m metrics\u001B[38;5;241m.\u001B[39maccuracy(test_input, misleading_output, grader\u001B[38;5;241m=\u001B[39mgrader)\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m misleading_result \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m4\u001B[39m, misleading_result\n",
      "\u001B[1;31mAssertionError\u001B[0m: 0.0"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-23T12:43:24.136123Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "5c9ab560813b91c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
