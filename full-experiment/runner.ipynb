{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "md# Explingo Experiment Runner\n",
    "\n",
    "This notebook:\n",
    "1. Loads the gold-standard dataset, prepares the metrics functions, and verifies that the metric functions give the maximum score on the gold-standard dataset, and lower scores on less aligned datasets\n",
    "2. Runs the prompt-design, few-shot, and bootstrap-few-shot experiments on a testing dataset"
   ],
   "id": "a9c2d95909e76143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Import necessary libraries, prepare the LLM, and load the gold-standard dataset\n",
    "\n",
    "**Note: To run these cells, you need a `keys.yaml` file in the top-level Explingo directory with the following line:**\n",
    "```yaml\n",
    "openai_api_key: <your_openai_api_key>\n",
    "```"
   ],
   "id": "743e31b8c011c24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-27T17:41:35.589004Z",
     "start_time": "2024-08-27T17:41:33.816492Z"
    }
   },
   "source": [
    "import examples, core \n",
    "import os\n",
    "import yaml\n",
    "import dspy\n",
    "import metrics\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "with open(os.path.join(\"..\", \"keys.yaml\"), \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "\n",
    "llm = dspy.OpenAI(model='gpt-4o', api_key=openai_api_key, max_tokens=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T17:41:35.604906Z",
     "start_time": "2024-08-27T17:41:35.590881Z"
    }
   },
   "id": "6e9d56b95001b3ac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some examples include gold-standard narratives; others include only a sample explanation.\n",
    "- The former makes up the gold-standard dataset used for tuning the evaluation metrics and providing few-shot examples.\n",
    "- The latter makes up the testing dataset used for evaluation and for bootstrapping few-shot examples"
   ],
   "id": "70871e15ba810c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T17:41:35.951414Z",
     "start_time": "2024-08-27T17:41:35.606902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labeled_train, labeled_eval, unlabeled_train, unlabeled_eval = examples.get_data(\"gold_standards.json\")\n",
    "train_data = labeled_train + unlabeled_train\n",
    "eval_data = labeled_eval + unlabeled_eval\n",
    "\n",
    "max_optimal_length = max([len(d.narrative) for d in labeled_train])\n",
    "max_optimal_length"
   ],
   "id": "66031abf3885cb9a",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gold_standards.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m labeled_train, labeled_eval, unlabeled_train, unlabeled_eval \u001B[38;5;241m=\u001B[39m \u001B[43mexamples\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgold_standards.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m train_data \u001B[38;5;241m=\u001B[39m labeled_train \u001B[38;5;241m+\u001B[39m unlabeled_train\n\u001B[0;32m      3\u001B[0m eval_data \u001B[38;5;241m=\u001B[39m labeled_eval \u001B[38;5;241m+\u001B[39m unlabeled_eval\n",
      "File \u001B[1;32m~\\Documents\\github\\Explingo\\full-experiment\\examples.py:27\u001B[0m, in \u001B[0;36mget_data\u001B[1;34m(json_file, split)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_data\u001B[39m(json_file, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.6\u001B[39m):\n\u001B[1;32m---> 27\u001B[0m     all_data \u001B[38;5;241m=\u001B[39m \u001B[43mload_examples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjson_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m     labeled_data \u001B[38;5;241m=\u001B[39m [example \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m all_data \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(example, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnarrative\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[0;32m     29\u001B[0m     unlabeled_data \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     30\u001B[0m         example \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m all_data \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(example, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnarrative\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     31\u001B[0m     ]\n",
      "File \u001B[1;32m~\\Documents\\github\\Explingo\\full-experiment\\examples.py:19\u001B[0m, in \u001B[0;36mload_examples\u001B[1;34m(json_file)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_examples\u001B[39m(json_file):\n\u001B[1;32m---> 19\u001B[0m     training_data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mjson_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     20\u001B[0m     examples \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m convo \u001B[38;5;129;01min\u001B[39;00m training_data:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'gold_standards.json'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we set up the evaluation metrics. We use the following metrics, all scored on a scale from 0-2:\n",
    "- Accuracy: the narrative accurately describes the information in the explanation\n",
    "- Fluency: the narrative is coherent and natural, as compared to the gold-standard explanations. We pass in a small list of sample narratives from the gold-standard dataset to compare against\n",
    "- Conciseness: the narrative is not too long, as compared to the gold-standard explanations. For now, any narrative that is no longer than the longest gold-standard narrative will score 2\n",
    "- Completeness: the narrative includes all relevant information from the original explanation \n",
    "\n",
    "**Note: You can set `verbose=1` to see the narratives generated, or `verbose=2` to see the explanations, narratives, and rationalizations**"
   ],
   "id": "733553c62a1d557d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "example_good_narratives = random.sample([d.narrative for d in labeled_train], 5)\n",
    "\n",
    "exp_metrics = metrics.Metrics(\n",
    "            [\n",
    "                metrics.accuracy,\n",
    "                metrics.fluency,\n",
    "                metrics.conciseness,\n",
    "                metrics.completeness,\n",
    "            ], verbose=0, openai_key=openai_api_key,\n",
    "        metric_kwargs={\"conciseness\": {\"max_optimal_length\": max_optimal_length},\n",
    "                       \"fluency\": {\"good_narratives\": example_good_narratives}}\n",
    "        )"
   ],
   "id": "75a9fcd3ad0e9975",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, we set up the main experiment runner object"
   ],
   "id": "aa44b11781eaddb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "explingo = core.Explingo(llm=llm, context=\"The model predicts house prices\",\n",
    "                         labeled_train_data=labeled_train, unlabeled_train_data=unlabeled_train)"
   ],
   "id": "9ac27ac4e6ca2496",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic prompt design experiment\n",
    "\n",
    "We begin with basic prompts. With 4 metrics (without completeness), each with a score of 0-2, the maximum score is 8. \n",
    "\n",
    "We generate narratives/rationalizations on `max_iters=5` sample explanations, and return the average total score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5694bfd440f310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Utility for cleaner results\n",
    "\n",
    "def pretty_print(result):\n",
    "    s = f\"Total score: {result[0]}\"\n",
    "    s2 = \", \".join([f\"{k}: {v}\" for k, v in result[1].items()])\n",
    "    print(f\"{s} ({s2})\")"
   ],
   "id": "7b3f0abc65532f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    \"You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\",\n",
    "    \"You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\",\n",
    "    \"You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    pretty_print(explingo.run_basic_prompting_experiment(eval_data, prompt=prompt, max_iters=5))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1be61df6a728fa29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few-shot experiment\n",
    "\n",
    "Next, we repeat the experiment with the addition of N few-shot examples from the gold-standard dataset."
   ],
   "id": "89d7ba7e0085534b"
  },
  {
   "cell_type": "code",
   "source": [
    "for i in [1, 3, 5]:\n",
    "    print(f\"Few-shot n: {i}\")\n",
    "    pretty_print(explingo.run_few_shot_experiment(eval_data, max_iters=5, n_few_shot=i))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c57212f361799ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bootstrapped few-shot\n",
    "Next, we repeat the experiment with the addition of 3 examples bootstrapped by DSPy to optimize the evaluation metrics."
   ],
   "id": "6e0ff9cf578f5b7b"
  },
  {
   "cell_type": "code",
   "source": [
    "for i, j in [[0, 3], [0, 5], [3, 3], [3, 5]]:\n",
    "    print(f\"Few-shot n: {i}, Bootstrapped n: {j}\")\n",
    "    pretty_print(explingo.run_bootstrap_few_shot_experiment(eval_data, max_iters=5, n_labeled_few_shot=i, n_bootstrapped_few_shot=j))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bb946a284aff621",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
