{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explingo Experiment Runner\n",
    "\n",
    "This notebook:\n",
    "1. Loads the gold-standard dataset, prepares the metrics functions, and verifies that the metric functions give the maximum score on the gold-standard dataset, and lower scores on less aligned datasets\n",
    "2. Runs the prompt-design, few-shot, and bootstrap-few-shot experiments on a testing dataset"
   ],
   "id": "a9c2d95909e76143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Import necessary libraries and prepare the LLM\n",
    "\n",
    "**Note: To run these cells, you need a `keys.yaml` file in the top-level Explingo directory with the following line:**\n",
    "```yaml\n",
    "openai_api_key: <your_openai_api_key>\n",
    "```"
   ],
   "id": "743e31b8c011c24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-06T21:22:09.584177Z",
     "start_time": "2024-09-06T21:22:08.204897Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from experiment_runner import ExplingoExperimentRunner\n",
    "import os\n",
    "import yaml\n",
    "import dspy\n",
    "import metrics\n",
    "import random\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "with open(os.path.join(\"..\", \"keys.yaml\"), \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "\n",
    "llm = dspy.OpenAI(model='gpt-4o', api_key=openai_api_key, max_tokens=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-06T21:22:09.599269Z",
     "start_time": "2024-09-06T21:22:09.586170Z"
    }
   },
   "id": "6e9d56b95001b3ac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we create the main experiment runner object. This object takes in a dataset, and then\n",
    "1. Splits the dataset into a training dataset and a testing dataset (see notes below)\n",
    "2. Sets up the evaluation metrics (see notes below). The fluency metric is set up to use sample from the dataset as reference\n",
    "3. Runs the experiments on the testing dataset"
   ],
   "id": "36792a8e050d0f6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some examples in the testing datasets include gold-standard narratives; others include only a sample explanation.\n",
    "- The former makes up the gold-standard dataset used for tuning the evaluation metrics and providing few-shot examples.\n",
    "- The latter makes up the testing dataset used for evaluation and for bootstrapping few-shot examples"
   ],
   "id": "70871e15ba810c46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We use the following metrics, all scored on a scale from 0-4:\n",
    "- Accuracy: the narrative accurately describes the information in the explanation\n",
    "- Fluency: the narrative is coherent and natural, as compared to the gold-standard explanations. We pass in a small list of sample narratives from the gold-standard dataset to compare against\n",
    "- Conciseness: the narrative is not too long, as compared to the gold-standard explanations. For now, any narrative that is no longer than the longest gold-standard narrative will score 4\n",
    "- Completeness: the narrative includes all relevant information from the original explanation \n",
    "\n",
    "**Note: You can set `verbose=1` to see the narratives generated, or `verbose=2` to see the explanations, narratives, and rationalizations**"
   ],
   "id": "733553c62a1d557d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T21:22:09.756699Z",
     "start_time": "2024-09-06T21:22:09.601762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# iterate all datasets in the eval_data folder\n",
    "runners = {}\n",
    "total_eval = 0\n",
    "for dataset in os.listdir(os.path.join(\"eval_data\")):\n",
    "    runners[dataset] = ExplingoExperimentRunner(llm=llm, openai_api_key=openai_api_key, dataset_filepath = os.path.join(\"eval_data\", dataset), verbose=0)\n",
    "    total_eval += len(runners[dataset].eval_data)\n",
    "    \n",
    "print(\"Total eval examples:\", total_eval)\n",
    "results = []\n",
    "narratives = []"
   ],
   "id": "9ac27ac4e6ca2496",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_data\\housing_1.json\n",
      "Total number of examples: 35\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 15\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 10\n",
      "Max optimal length: 12.24\n",
      "---\n",
      "eval_data\\housing_2.json\n",
      "Total number of examples: 22\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 7\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 5\n",
      "Max optimal length: 9.0\n",
      "---\n",
      "eval_data\\housing_3.json\n",
      "Total number of examples: 22\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 8\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 4\n",
      "Max optimal length: 6.84\n",
      "---\n",
      "eval_data\\mushroom_1.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 6\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 14\n",
      "Max optimal length: 9.9\n",
      "---\n",
      "eval_data\\mushroom_2.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 6\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 14\n",
      "Max optimal length: 8.4\n",
      "---\n",
      "eval_data\\pdf_1.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 4\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 16\n",
      "Max optimal length: 8.4\n",
      "---\n",
      "eval_data\\pdf_2.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 4\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 16\n",
      "Max optimal length: 6.6\n",
      "---\n",
      "eval_data\\student_1.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 10\n",
      "Labeled evaluation examples: 20\n",
      "Unlabeled training examples: 0\n",
      "Unlabeled evaluation examples: 0\n",
      "Max optimal length: 12.9\n",
      "---\n",
      "eval_data\\student_2.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 9\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 11\n",
      "Max optimal length: 9.6\n",
      "---\n",
      "Total eval examples: 169\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T21:22:09.772700Z",
     "start_time": "2024-09-06T21:22:09.757702Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoint = 0",
   "id": "37e42840e8a0d1d0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic prompt design experiment\n",
    "\n",
    "We begin with basic prompts. With 4 metrics (without completeness), each with a score of 0-2, the maximum score is 8. \n",
    "\n",
    "We generate narratives/rationalizations on `max_iters=5` sample explanations, and return the average total score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5694bfd440f310"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T21:22:09.788717Z",
     "start_time": "2024-09-06T21:22:09.773701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utilities for cleaner results\n",
    "\n",
    "def pretty_print(result):\n",
    "    s = f\"Total score: {result[0]}\"\n",
    "    s2 = \", \".join([f\"{k}: {v}\" for k, v in result[1].items()])\n",
    "    print(f\"{s} ({s2})\")\n",
    "    \n",
    "def update_results(method, dataset, scores, kwargs):\n",
    "    result = {\"dataset\": dataset, \"total score\": scores[0]}\n",
    "    result.update(scores[1])\n",
    "    result.update(kwargs)\n",
    "    results.append(result)\n",
    "    "
   ],
   "id": "7b3f0abc65532f7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = [\"You are helping users understand an ML model's prediction. Given an explanation and information about the model, \"\n",
    "           \"convert the explanation into a human-readable narrative.\",\n",
    "           \"You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\",\n",
    "           \"You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\",\n",
    "]\n",
    "\n",
    "for dataset in runners:\n",
    "    runner = runners[dataset]\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        scores = runner.run_basic_prompting_experiment(prompt=prompt, max_iters=5)\n",
    "        update_results(\"basic_prompting\", dataset, scores, {\"prompt\": prompt})\n",
    "        narratives.append(scores[2])\n",
    "        pretty_print(scores)\n",
    "        print(\"--\")\n",
    "    print(\"DUMPING...\")\n",
    "    with open(f'results_checkpoint.json', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    with open(f'narratives_checkpoint.json', 'w') as f:\n",
    "        json.dump(narratives, f)\n",
    "    print(\"DUMPING COMPLETE\")\n",
    "    print(\"=====\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-06T21:23:06.878587Z",
     "start_time": "2024-09-06T21:22:09.789734Z"
    }
   },
   "id": "1be61df6a728fa29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: housing_1.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 13.39607843137255 (accuracy: 4.0, completeness: 4.0, fluency: 3.2, conciseness: 2.1960784313725497)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 13.27450980392157 (accuracy: 4.0, completeness: 4.0, fluency: 3.0, conciseness: 2.274509803921569)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 14.01045751633987 (accuracy: 4.0, completeness: 4.0, fluency: 3.2, conciseness: 2.8104575163398695)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n",
      "Dataset: housing_2.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 11.08 (accuracy: 4.0, completeness: 4.0, fluency: 2.6, conciseness: 0.4800000000000001)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 11.16 (accuracy: 4.0, completeness: 4.0, fluency: 2.2, conciseness: 0.96)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 11.142222222222221 (accuracy: 4.0, completeness: 4.0, fluency: 2.2, conciseness: 0.9422222222222221)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n",
      "Dataset: housing_3.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 9.8 (accuracy: 4.0, completeness: 4.0, fluency: 1.8, conciseness: 0.0)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 10.2 (accuracy: 4.0, completeness: 4.0, fluency: 2.2, conciseness: 0.0)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 9.8 (accuracy: 4.0, completeness: 4.0, fluency: 1.8, conciseness: 0.0)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n",
      "Dataset: mushroom_1.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 10.896969696969697 (accuracy: 4.0, completeness: 4.0, fluency: 1.2, conciseness: 1.6969696969696977)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 10.816835016835018 (accuracy: 4.0, completeness: 4.0, fluency: 1.4, conciseness: 1.4168350168350172)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 10.57777777777778 (accuracy: 4.0, completeness: 4.0, fluency: 0.8, conciseness: 1.7777777777777786)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n",
      "Dataset: mushroom_2.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 10.171428571428573 (accuracy: 4.0, completeness: 4.0, fluency: 1.6, conciseness: 0.5714285714285721)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 10.126984126984128 (accuracy: 4.0, completeness: 4.0, fluency: 1.6, conciseness: 0.5269841269841276)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 10.136507936507936 (accuracy: 4.0, completeness: 4.0, fluency: 1.4, conciseness: 0.7365079365079369)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n",
      "Dataset: pdf_1.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 11.065079365079365 (accuracy: 4.0, completeness: 4.0, fluency: 3.0, conciseness: 0.06507936507936521)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 11.2 (accuracy: 4.0, completeness: 4.0, fluency: 3.2, conciseness: 0.0)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 11.066666666666666 (accuracy: 4.0, completeness: 4.0, fluency: 2.8, conciseness: 0.2666666666666668)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n",
      "Dataset: pdf_2.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 10.6 (accuracy: 4.0, completeness: 4.0, fluency: 2.6, conciseness: 0.0)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 10.8 (accuracy: 4.0, completeness: 4.0, fluency: 2.8, conciseness: 0.0)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 10.0 (accuracy: 4.0, completeness: 4.0, fluency: 2.0, conciseness: 0.0)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n",
      "Dataset: student_1.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 13.08940568475452 (accuracy: 4.0, completeness: 4.0, fluency: 2.8, conciseness: 2.2894056847545223)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 11.918863049095608 (accuracy: 3.2, completeness: 4.0, fluency: 2.6, conciseness: 2.1188630490956077)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 13.127131782945735 (accuracy: 4.0, completeness: 4.0, fluency: 2.6, conciseness: 2.5271317829457365)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n",
      "Dataset: student_2.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 11.755555555555555 (accuracy: 4.0, completeness: 4.0, fluency: 3.4, conciseness: 0.355555555555555)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 11.75 (accuracy: 4.0, completeness: 4.0, fluency: 3.2, conciseness: 0.5499999999999996)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 11.63888888888889 (accuracy: 4.0, completeness: 4.0, fluency: 3.2, conciseness: 0.4388888888888884)\n",
      "--\n",
      "DUMPING...\n",
      "DUMPING COMPLETE\n",
      "=====\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few-shot experiment\n",
    "\n",
    "Next, we repeat the experiment with the addition of N few-shot examples from the gold-standard dataset."
   ],
   "id": "89d7ba7e0085534b"
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in runners:\n",
    "    runner = runners[dataset]\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for i in [1, 3, 5]:\n",
    "        print(f\"Few-shot n: {i}\")\n",
    "        scores = runner.run_few_shot_experiment(n_few_shot=i, prompt=prompts[0], max_iters=5)\n",
    "        update_results(\"few_shot\", dataset, scores, {\"n_few_shot\": i, \"prompt\": prompts[0]})\n",
    "        narratives.append(scores[2])\n",
    "        pretty_print(scores)\n",
    "        print(\"--\")\n",
    "    with open(f'results_checkpoint.json', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    with open(f'narratives_checkpoint.json', 'w') as f:\n",
    "        json.dump(narratives, f)\n",
    "    print(\"=====\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-06T21:28:32.962947Z",
     "start_time": "2024-09-06T21:23:06.880533Z"
    }
   },
   "id": "c57212f361799ac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: housing_1.json\n",
      "Few-shot n: 1\n",
      "Total score: 15.775163398692811 (accuracy: 4.0, completeness: 4.0, fluency: 4.0, conciseness: 3.7751633986928113)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 15.670588235294119 (accuracy: 4.0, completeness: 4.0, fluency: 4.0, conciseness: 3.670588235294118)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Total score: 15.722875816993465 (accuracy: 4.0, completeness: 4.0, fluency: 4.0, conciseness: 3.722875816993464)\n",
      "--\n",
      "=====\n",
      "Dataset: housing_2.json\n",
      "Few-shot n: 1\n",
      "Total score: 15.426666666666666 (accuracy: 4.0, completeness: 4.0, fluency: 3.8, conciseness: 3.6266666666666665)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 15.00888888888889 (accuracy: 4.0, completeness: 3.6, fluency: 3.8, conciseness: 3.608888888888889)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Total score: 15.48 (accuracy: 4.0, completeness: 4.0, fluency: 3.8, conciseness: 3.6800000000000006)\n",
      "--\n",
      "=====\n",
      "Dataset: housing_3.json\n",
      "Few-shot n: 1\n",
      "Total score: 15.497076023391813 (accuracy: 4.0, completeness: 3.6, fluency: 4.0, conciseness: 3.8970760233918127)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 15.174269005847952 (accuracy: 4.0, completeness: 3.6, fluency: 4.0, conciseness: 3.574269005847954)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Total score: 15.492397660818714 (accuracy: 4.0, completeness: 3.6, fluency: 4.0, conciseness: 3.892397660818714)\n",
      "--\n",
      "=====\n",
      "Dataset: mushroom_1.json\n",
      "Few-shot n: 1\n",
      "Total score: 13.72996632996633 (accuracy: 3.2, completeness: 3.6, fluency: 3.0, conciseness: 3.92996632996633)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 13.72996632996633 (accuracy: 3.2, completeness: 3.6, fluency: 3.0, conciseness: 3.92996632996633)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Unable to parse output: This mushroom is more likely to be poisonous because it has no odor, a broad gill size, and a black spore print color. Be cautious!\n",
      "Total score: 11.406734006734007 (accuracy: 3.2, completeness: 2.8, fluency: 2.6, conciseness: 2.8067340067340067)\n",
      "--\n",
      "=====\n",
      "Dataset: mushroom_2.json\n",
      "Few-shot n: 1\n",
      "Total score: 13.095238095238097 (accuracy: 2.4, completeness: 4.0, fluency: 3.0, conciseness: 3.6952380952380954)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 11.511111111111111 (accuracy: 0.8, completeness: 3.2, fluency: 3.6, conciseness: 3.9111111111111114)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Total score: 11.311111111111112 (accuracy: 0.8, completeness: 3.2, fluency: 3.4, conciseness: 3.9111111111111114)\n",
      "--\n",
      "=====\n",
      "Dataset: pdf_1.json\n",
      "Few-shot n: 1\n",
      "Total score: 13.780952380952382 (accuracy: 4.0, completeness: 2.4, fluency: 3.8, conciseness: 3.5809523809523816)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 13.780952380952382 (accuracy: 4.0, completeness: 2.4, fluency: 3.8, conciseness: 3.5809523809523816)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Total score: 13.780952380952382 (accuracy: 4.0, completeness: 2.4, fluency: 3.8, conciseness: 3.5809523809523816)\n",
      "--\n",
      "=====\n",
      "Dataset: pdf_2.json\n",
      "Few-shot n: 1\n",
      "Total score: 10.484848484848484 (accuracy: 0.0, completeness: 2.8, fluency: 4.0, conciseness: 3.6848484848484846)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 10.444444444444445 (accuracy: 0.0, completeness: 2.8, fluency: 4.0, conciseness: 3.6444444444444444)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Total score: 10.525252525252526 (accuracy: 0.0, completeness: 2.8, fluency: 4.0, conciseness: 3.7252525252525253)\n",
      "--\n",
      "=====\n",
      "Dataset: student_1.json\n",
      "Few-shot n: 1\n",
      "Total score: 14.531782945736433 (accuracy: 4.0, completeness: 3.6, fluency: 3.0, conciseness: 3.931782945736434)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 14.6 (accuracy: 4.0, completeness: 3.6, fluency: 3.0, conciseness: 4.0)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Total score: 14.37312661498708 (accuracy: 4.0, completeness: 3.2, fluency: 3.2, conciseness: 3.9731266149870805)\n",
      "--\n",
      "=====\n",
      "Dataset: student_2.json\n",
      "Few-shot n: 1\n",
      "Total score: 15.216666666666665 (accuracy: 4.0, completeness: 3.6, fluency: 4.0, conciseness: 3.6166666666666663)\n",
      "--\n",
      "Few-shot n: 3\n",
      "Total score: 15.45 (accuracy: 4.0, completeness: 4.0, fluency: 4.0, conciseness: 3.45)\n",
      "--\n",
      "Few-shot n: 5\n",
      "Total score: 15.327777777777778 (accuracy: 4.0, completeness: 3.6, fluency: 4.0, conciseness: 3.727777777777778)\n",
      "--\n",
      "=====\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bootstrapped few-shot\n",
    "Next, we repeat the experiment with the addition of 3 examples bootstrapped by DSPy to optimize the evaluation metrics."
   ],
   "id": "6e0ff9cf578f5b7b"
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in runners:\n",
    "    runner = runners[dataset]\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for i, j in [[1, 1], [1, 3], [3, 1], [3, 3]]:\n",
    "        print(f\"Few-shot n: {i}, Bootstrapped n: {j}\")\n",
    "        scores = runner.run_bootstrap_few_shot_experiment(n_labeled_few_shot=i, n_bootstrapped_few_shot=j, max_iters=5)\n",
    "        update_results(\"bootstrap_few_shot\", dataset, scores, {\"n_few_shot\": i, \"n_bootstrapped_few_shot\": j, \"prompt\": prompts[0]})\n",
    "        narratives.append(scores[2])\n",
    "        pretty_print(scores)\n",
    "        print(\"--\")\n",
    "    with open(f'results_checkpoint.json', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    with open(f'narratives_checkpoint.json', 'w') as f:\n",
    "        json.dump(narratives, f)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-06T21:28:32.963946Z"
    }
   },
   "id": "7bb946a284aff621",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: housing_1.json\n",
      "Few-shot n: 1, Bootstrapped n: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 0 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 253.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 0 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 270.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 0 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 263.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 0 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 219.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 0 full traces after 10 examples in round 0.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# result_df = pd.DataFrame(results)\n",
    "# result_df.to_csv(\"results.csv\")\n",
    "# result_df"
   ],
   "id": "8c9376972f39a1bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "b3ed31a97e1a6e19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
