{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explingo Experiment Runner\n",
    "\n",
    "This notebook:\n",
    "1. Loads the gold-standard dataset, prepares the metrics functions, and verifies that the metric functions give the maximum score on the gold-standard dataset\n",
    "TODO: ensure that narratives very unlike gold-standard score lower\n",
    "2. Runs the prompt-design, few-shot, and bootstrap-few-shot experiments on a testing dataset"
   ],
   "id": "a9c2d95909e76143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Import necessary libraries, prepare the LLM, and load the gold-standard dataset\n",
    "\n",
    "**Note: To run these cells, you need a `keys.yaml` file in the top-level Explingo directory with the following line:**\n",
    "```yaml\n",
    "openai_api_key: <your_openai_api_key>\n",
    "```"
   ],
   "id": "743e31b8c011c24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-20T19:17:11.775244Z",
     "start_time": "2024-08-20T19:17:11.775244Z"
    }
   },
   "source": [
    "import examples, core \n",
    "import os\n",
    "import yaml\n",
    "import dspy\n",
    "import metrics\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with open(os.path.join(\"..\", \"keys.yaml\"), \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "\n",
    "llm = dspy.OpenAI(model='gpt-4o', api_key=openai_api_key, max_tokens=2000)\n",
    "\n",
    "data = examples.load_examples(\"examples.json\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9d56b95001b3ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some examples include gold-standard narratives; others include only a sample explanation.\n",
    "- The former makes up the gold-standard dataset used for tuning the evaluation metrics and providing few-shot examples.\n",
    "- The latter makes up the testing dataset used for evaluation and for bootstrapping few-shot examples\n",
    "\n",
    "TODO: Right now, we few-shot prompt and evaluation using the same dataset which may lead to biased results, we should separate into a test and train dataset"
   ],
   "id": "70871e15ba810c46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gold_standards = [d for d in data if hasattr(d, \"narrative\")]\n",
    "max_optimal_length = max([len(d.narrative) for d in gold_standards])\n",
    "max_optimal_length"
   ],
   "id": "66031abf3885cb9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we set up the evaluation metrics. We use the following metrics, all scored on a scale from 0-2:\n",
    "- Accuracy: the narrative accurately describes the information in the explanation\n",
    "- Fluency: the narrative is coherent and natural, as compared to the gold-standard explanations. We pass in a small list of sample narratives from the gold-standard dataset to compare against\n",
    "- Conciseness: the narrative is not too long, as compared to the gold-standard explanations. For now, any narrative that is no longer than the longest gold-standard narrative will score 2\n",
    "- Context Awareness: the rationalization given alongside the explanation is relevant\n",
    "- Completeness: the narrative includes all relevant information from the original explanation \n",
    "\n",
    "TODO: Currently, we have removed the completeness metric as the accuracy metric ends up encompassing it (and the definition of complete depends on the gold-standard). We need to decide if we want to add it back in"
   ],
   "id": "733553c62a1d557d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "example_good_narratives = random.sample([d.narrative for d in gold_standards], 5)\n",
    "example_bad_narratives = random.sample([d.bad_narrative for d in gold_standards], 5)\n",
    "\n",
    "exp_metrics = metrics.Metrics(\n",
    "            [\n",
    "                metrics.accuracy,\n",
    "                metrics.fluency,\n",
    "                metrics.conciseness,\n",
    "                metrics.context_awareness,\n",
    "            ], verbose=0, \n",
    "        metric_kwargs={\"conciseness\": {\"max_optimal_length\": max_optimal_length},\n",
    "                       \"fluency\": {\"good_narratives\": example_good_narratives, \"bad_narratives\": example_bad_narratives}}\n",
    "        )"
   ],
   "id": "75a9fcd3ad0e9975",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we set up the main experiment runner object",
   "id": "aa44b11781eaddb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "explingo = core.Explingo(llm=llm, context=\"The model predicts house prices\", \n",
    "                         examples=data, metric=exp_metrics)"
   ],
   "id": "9ac27ac4e6ca2496",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Verifying the Evaluation Metrics\n",
    "\n",
    "In the following code block, we verify that all the evaluation metrics give the maximum score on the gold-standard dataset. We are defining a successful narrative as one that matches the standards of the gold-standard dataset; this allows us to ensure that the metrics are aligned to this gold-standard.\n",
    "\n",
    "**Note: occasionally, we do see a few failures, but for now most runs I am seeing 100% success.**\n",
    "\n",
    "TODO: To complete this, we also need to ensure that narratives that are unlike the gold-standard score lower, using a separate dataset\n",
    "\n",
    "TODO: We do not currently verify the context awareness metric, as the gold-standard dataset does not include a rationalization"
   ],
   "id": "f370ebdbbdf7fe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VERIFY = True\n",
    "\n",
    "if VERIFY:\n",
    "    # Gold standard dataset does not include a rationalization, so we skip context awareness \n",
    "    ver_metrics = metrics.Metrics(\n",
    "        [\n",
    "            metrics.accuracy,\n",
    "            metrics.fluency,\n",
    "            metrics.conciseness,\n",
    "        ], verbose=0, \n",
    "        metric_kwargs={\"conciseness\": {\"max_optimal_length\": max_optimal_length},\n",
    "                       \"fluency\": {\"good_narratives\": example_good_narratives, \"bad_narratives\": example_bad_narratives}}\n",
    "    )\n",
    "    total_passed, total_failed = 0, 0\n",
    "    for gold_standard in gold_standards:\n",
    "        result = ver_metrics(gold_standard, gold_standard)[0]\n",
    "        if result != 2*len(ver_metrics.metric_funcs):\n",
    "            print(f\"Failed {gold_standard}\")\n",
    "            total_failed += 1\n",
    "        else:\n",
    "            total_passed += 1\n",
    "    print(f\"Total passed: {total_passed}, total failed: {total_failed}\")"
   ],
   "id": "4b38609ff7969130",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BASIC PROMPTING\n",
    "\n",
    "We begin with basic prompts. With 4 metrics (without completeness), each with a score of 0-2, the maximum score is 8. \n",
    "\n",
    "We generate narratives/rationalizations on `max_iters=5` sample explanations, and return the average total score.\n",
    "\n",
    "TODO: We need to repeat this for each of our 5 test prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5694bfd440f310"
  },
  {
   "cell_type": "code",
   "source": "explingo.run_experiment(data, prompt_type=\"basic\", max_iters=5)",
   "metadata": {
    "collapsed": false
   },
   "id": "1be61df6a728fa29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we repeat the experiment with the addition of 3 few-shot examples from the gold-standard dataset.",
   "id": "89d7ba7e0085534b"
  },
  {
   "cell_type": "code",
   "source": [
    "for i in [1, 3, 5]:\n",
    "    print(f\"Few-shot n: {i}\")\n",
    "    print(explingo.run_experiment(data, prompt_type=\"few-shot\", max_iters=5, few_shot_n=i))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c57212f361799ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we repeat the experiment with the addition of 3 examples bootstrapped by DSPy to optimize the evaluation metrics.\n",
    "\n",
    "TODO: We should experiment with different numbers of labeled few-shot and bootstrapped few-shot examples"
   ],
   "id": "6e0ff9cf578f5b7b"
  },
  {
   "cell_type": "code",
   "source": "explingo.run_experiment(data, prompt_type=\"bootstrap-few-shot\", max_iters=5, few_shot_n=3)",
   "metadata": {
    "collapsed": false
   },
   "id": "7bb946a284aff621",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
