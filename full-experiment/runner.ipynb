{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explingo Experiment Runner\n",
    "\n",
    "This notebook:\n",
    "1. Loads the gold-standard dataset, prepares the metrics functions, and verifies that the metric functions give the maximum score on the gold-standard dataset, and lower scores on less aligned datasets\n",
    "2. Runs the prompt-design, few-shot, and bootstrap-few-shot experiments on a testing dataset"
   ],
   "id": "a9c2d95909e76143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Import necessary libraries and prepare the LLM\n",
    "\n",
    "**Note: To run these cells, you need a `keys.yaml` file in the top-level Explingo directory with the following line:**\n",
    "```yaml\n",
    "openai_api_key: <your_openai_api_key>\n",
    "```"
   ],
   "id": "743e31b8c011c24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-30T13:19:07.497796Z",
     "start_time": "2024-08-30T13:19:07.494794Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from experiment_runner import ExplingoExperimentRunner\n",
    "import os\n",
    "import yaml\n",
    "import dspy\n",
    "import metrics\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "with open(os.path.join(\"..\", \"keys.yaml\"), \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "\n",
    "llm = dspy.OpenAI(model='gpt-4o', api_key=openai_api_key, max_tokens=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-30T13:19:07.516813Z",
     "start_time": "2024-08-30T13:19:07.513405Z"
    }
   },
   "id": "6e9d56b95001b3ac",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we create the main experiment runner object. This object takes in a dataset, and then\n",
    "1. Splits the dataset into a training dataset and a testing dataset (see notes below)\n",
    "2. Sets up the evaluation metrics (see notes below). The fluency metric is set up to use sample from the dataset as reference\n",
    "3. Runs the experiments on the testing dataset"
   ],
   "id": "36792a8e050d0f6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some examples in the testing datasets include gold-standard narratives; others include only a sample explanation.\n",
    "- The former makes up the gold-standard dataset used for tuning the evaluation metrics and providing few-shot examples.\n",
    "- The latter makes up the testing dataset used for evaluation and for bootstrapping few-shot examples"
   ],
   "id": "70871e15ba810c46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We use the following metrics, all scored on a scale from 0-4:\n",
    "- Accuracy: the narrative accurately describes the information in the explanation\n",
    "- Fluency: the narrative is coherent and natural, as compared to the gold-standard explanations. We pass in a small list of sample narratives from the gold-standard dataset to compare against\n",
    "- Conciseness: the narrative is not too long, as compared to the gold-standard explanations. For now, any narrative that is no longer than the longest gold-standard narrative will score 4\n",
    "- Completeness: the narrative includes all relevant information from the original explanation \n",
    "\n",
    "**Note: You can set `verbose=1` to see the narratives generated, or `verbose=2` to see the explanations, narratives, and rationalizations**"
   ],
   "id": "733553c62a1d557d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T13:19:07.587750Z",
     "start_time": "2024-08-30T13:19:07.517821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# iterate all datasets in the eval_data folder\n",
    "runners = {}\n",
    "total_eval = 0\n",
    "for dataset in os.listdir(os.path.join(\"eval_data\")):\n",
    "    runners[dataset] = ExplingoExperimentRunner(llm=llm, openai_api_key=openai_api_key, dataset_filepath = os.path.join(\"eval_data\", dataset), verbose=0)\n",
    "    total_eval += len(runners[dataset].eval_data)\n",
    "    \n",
    "print(\"Total eval examples:\", total_eval)\n",
    "results = []"
   ],
   "id": "9ac27ac4e6ca2496",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_data\\housing_1.json\n",
      "Total number of examples: 35\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 15\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 10\n",
      "---\n",
      "eval_data\\housing_2.json\n",
      "Total number of examples: 22\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 7\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 5\n",
      "---\n",
      "eval_data\\housing_3.json\n",
      "Total number of examples: 22\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 8\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 4\n",
      "---\n",
      "eval_data\\mushroom_1.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 6\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 14\n",
      "---\n",
      "eval_data\\mushroom_2.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 6\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 14\n",
      "---\n",
      "eval_data\\pdf_1.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 4\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 16\n",
      "---\n",
      "eval_data\\pdf_2.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 4\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 16\n",
      "---\n",
      "eval_data\\student_1.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 10\n",
      "Labeled evaluation examples: 20\n",
      "Unlabeled training examples: 0\n",
      "Unlabeled evaluation examples: 0\n",
      "---\n",
      "eval_data\\student_2.json\n",
      "Total number of examples: 30\n",
      "Labeled training examples: 5\n",
      "Labeled evaluation examples: 9\n",
      "Unlabeled training examples: 5\n",
      "Unlabeled evaluation examples: 11\n",
      "---\n",
      "Total eval examples: 169\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic prompt design experiment\n",
    "\n",
    "We begin with basic prompts. With 4 metrics (without completeness), each with a score of 0-2, the maximum score is 8. \n",
    "\n",
    "We generate narratives/rationalizations on `max_iters=5` sample explanations, and return the average total score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5694bfd440f310"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T13:19:07.591540Z",
     "start_time": "2024-08-30T13:19:07.587750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utilities for cleaner results\n",
    "\n",
    "def pretty_print(result):\n",
    "    s = f\"Total score: {result[0]}\"\n",
    "    s2 = \", \".join([f\"{k}: {v}\" for k, v in result[1].items()])\n",
    "    print(f\"{s} ({s2})\")\n",
    "    \n",
    "def update_results(method, dataset, scores, kwargs):\n",
    "    result = {\"dataset\": dataset, \"prompt\": prompt, \"total score\": scores[0]}\n",
    "    result.update(scores[1])\n",
    "    result.update(kwargs)\n",
    "    results.append(result)"
   ],
   "id": "7b3f0abc65532f7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = [\"You are helping users understand an ML model's prediction. Given an explanation and information about the model, \"\n",
    "           \"convert the explanation into a human-readable narrative.\",\n",
    "           \"You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\",\n",
    "           \"You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\",\n",
    "]\n",
    "\n",
    "for dataset in runners:\n",
    "    runner = runners[dataset]\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        scores = runner.run_basic_prompting_experiment(prompt=prompt)\n",
    "        update_results(\"basic_prompting\", dataset, scores, {\"prompt\": prompt})\n",
    "        pretty_print(scores)\n",
    "        print(\"--\")\n",
    "    print(\"=====\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-30T13:36:25.849427Z",
     "start_time": "2024-08-30T13:19:07.592547Z"
    }
   },
   "id": "1be61df6a728fa29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: housing_1.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 12.56 (accuracy: 2.56, completeness: 3.76, fluency: 2.24, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 12.4 (accuracy: 2.4, completeness: 3.76, fluency: 2.24, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 12.8 (accuracy: 2.56, completeness: 4.0, fluency: 2.24, conciseness: 4.0)\n",
      "--\n",
      "=====\n",
      "Dataset: housing_2.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 14.833333333333334 (accuracy: 4.0, completeness: 4.0, fluency: 2.8333333333333335, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 14.666666666666666 (accuracy: 4.0, completeness: 4.0, fluency: 2.6666666666666665, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 15.083333333333334 (accuracy: 4.0, completeness: 4.0, fluency: 3.0833333333333335, conciseness: 4.0)\n",
      "--\n",
      "=====\n",
      "Dataset: housing_3.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 15.166666666666666 (accuracy: 4.0, completeness: 4.0, fluency: 3.1666666666666665, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 14.833333333333334 (accuracy: 4.0, completeness: 4.0, fluency: 2.8333333333333335, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 15.0 (accuracy: 4.0, completeness: 4.0, fluency: 3.0, conciseness: 4.0)\n",
      "--\n",
      "=====\n",
      "Dataset: mushroom_1.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 13.65 (accuracy: 4.0, completeness: 3.5, fluency: 2.15, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 13.65 (accuracy: 4.0, completeness: 3.2, fluency: 2.45, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 14.75 (accuracy: 4.0, completeness: 4.0, fluency: 2.75, conciseness: 4.0)\n",
      "--\n",
      "=====\n",
      "Dataset: mushroom_2.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 13.5 (accuracy: 4.0, completeness: 3.5, fluency: 2.0, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 13.35 (accuracy: 4.0, completeness: 3.2, fluency: 2.15, conciseness: 4.0)\n",
      "--\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 14.0 (accuracy: 4.0, completeness: 4.0, fluency: 2.0, conciseness: 4.0)\n",
      "--\n",
      "=====\n",
      "Dataset: pdf_1.json\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'The narrative mentions \"a high number of Javascript keywords,\" which corresponds to Feature 1 but does not specify the value (4.0) or the exact contribution (0.12). It also mentions \"the total size o",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrompt: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprompt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m     scores \u001B[38;5;241m=\u001B[39m \u001B[43mrunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_basic_prompting_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     update_results(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbasic_prompting\u001B[39m\u001B[38;5;124m\"\u001B[39m, dataset, scores, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt})\n\u001B[0;32m     14\u001B[0m     pretty_print(scores)\n",
      "File \u001B[1;32m~\\Documents\\github\\Explingo\\full-experiment\\experiment_runner.py:92\u001B[0m, in \u001B[0;36mrun_basic_prompting_experiment\u001B[1;34m(self, prompt, max_iters)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_basic_prompting_experiment\u001B[39m(\u001B[38;5;28mself\u001B[39m, prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, max_iters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m):\n\u001B[0;32m     82\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;124;03m    Run a basic prompting experiment\u001B[39;00m\n\u001B[0;32m     84\u001B[0m \n\u001B[0;32m     85\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;124;03m        prompt (string): Prompt\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;124;03m        max_iters (int): Maximum number of examples to run on\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \n\u001B[0;32m     89\u001B[0m \u001B[38;5;124;03m    Returns:\u001B[39;00m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;124;03m        total_average_score (float): Average total score over all explanations\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;124;03m        average_scores (pd.Series): Average scores for each metric\u001B[39;00m\n\u001B[1;32m---> 92\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_experiment(\n\u001B[0;32m     94\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplingo\u001B[38;5;241m.\u001B[39mbasic_prompt,\n\u001B[0;32m     95\u001B[0m         prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[0;32m     96\u001B[0m         max_iters\u001B[38;5;241m=\u001B[39mmax_iters,\n\u001B[0;32m     97\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\github\\Explingo\\full-experiment\\experiment_runner.py:68\u001B[0m, in \u001B[0;36mrun_experiment\u001B[1;34m(self, func, prompt, max_iters, kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m max_iters:\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     63\u001B[0m result \u001B[38;5;241m=\u001B[39m func(\n\u001B[0;32m     64\u001B[0m     prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[0;32m     65\u001B[0m     explanation\u001B[38;5;241m=\u001B[39mexample\u001B[38;5;241m.\u001B[39mexplanation,\n\u001B[0;32m     66\u001B[0m     explanation_format\u001B[38;5;241m=\u001B[39mexample\u001B[38;5;241m.\u001B[39mexplanation_format,\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m---> 68\u001B[0m )\n\u001B[0;32m     69\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics(example, result)\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m total_scores \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\github\\Explingo\\full-experiment\\metrics.py:46\u001B[0m, in \u001B[0;36mMetrics.__call__\u001B[1;34m(self, input_, output_, trace)\u001B[0m\n\u001B[0;32m     44\u001B[0m     metric_name \u001B[38;5;241m=\u001B[39m metric\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m     45\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetric_kwargs\u001B[38;5;241m.\u001B[39mget(metric_name, {})\n\u001B[1;32m---> 46\u001B[0m     metrics[metric_name] \u001B[38;5;241m=\u001B[39m \u001B[43mmetric\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m total_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(metrics\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[1;32m~\\Documents\\github\\Explingo\\full-experiment\\metrics.py:160\u001B[0m, in \u001B[0;36mcompleteness\u001B[1;34m(input_, output_, grader, trace)\u001B[0m\n\u001B[0;32m    154\u001B[0m rubric \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0 - One or more feature names from the explanation are not mentioned at all in the narrative. 2 - All features are mentioned, but not all feature values and/or contribution directions. 4 - All features are mentioned, and for each feature, includes at least an approximation of the feature\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms value and contribution direction.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    155\u001B[0m rational_type \u001B[38;5;241m=\u001B[39m dspy\u001B[38;5;241m.\u001B[39mOutputField(\n\u001B[0;32m    156\u001B[0m     prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStart by listing out all the features in the explanations, and then determine every feature is present in the narrative, along with its value and contribution direction.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    157\u001B[0m     desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature-by-feature processing of the narrative.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    158\u001B[0m )\n\u001B[1;32m--> 160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompute_score_from_rubric\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompleteness\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrubric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnarrative\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrational_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrational_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\github\\Explingo\\full-experiment\\metrics.py:111\u001B[0m, in \u001B[0;36mcompute_score_from_rubric\u001B[1;34m(metric, question, rubric, narrative, grader, iters, rational_type)\u001B[0m\n\u001B[0;32m    105\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    106\u001B[0m             score \u001B[38;5;241m=\u001B[39m dspy\u001B[38;5;241m.\u001B[39mChainOfThought(RubricAssess, rationale_type\u001B[38;5;241m=\u001B[39mrational_type)(\n\u001B[0;32m    107\u001B[0m                 question\u001B[38;5;241m=\u001B[39mquestion,\n\u001B[0;32m    108\u001B[0m                 rubric\u001B[38;5;241m=\u001B[39mrubric,\n\u001B[0;32m    109\u001B[0m                 narrative\u001B[38;5;241m=\u001B[39mnarrative,\n\u001B[0;32m    110\u001B[0m             )\u001B[38;5;241m.\u001B[39massessment\n\u001B[1;32m--> 111\u001B[0m         scores\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mscore\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01min\u001B[39;00m scores \u001B[38;5;129;01mand\u001B[39;00m MAX_SCORE \u001B[38;5;129;01min\u001B[39;00m scores:\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInconsistent score for metric \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (metric, scores))\n",
      "\u001B[1;31mValueError\u001B[0m: invalid literal for int() with base 10: 'The narrative mentions \"a high number of Javascript keywords,\" which corresponds to Feature 1 but does not specify the value (4.0) or the exact contribution (0.12). It also mentions \"the total size o"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few-shot experiment\n",
    "\n",
    "Next, we repeat the experiment with the addition of N few-shot examples from the gold-standard dataset."
   ],
   "id": "89d7ba7e0085534b"
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in runners:\n",
    "    runner = runners[dataset]\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for i in [1, 3, 5]:\n",
    "        print(f\"Few-shot n: {i}\")\n",
    "        scores = runner.run_few_shot_experiment(n_few_shot=i, prompt=prompts[0])\n",
    "        update_results(\"few_shot\", dataset, scores, {\"n_few_shot\": i, \"prompt\": prompts[0]})\n",
    "        pretty_print(scores)\n",
    "        print(\"--\")\n",
    "    print(\"=====\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-30T13:36:25.850433Z"
    }
   },
   "id": "c57212f361799ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-30T13:36:25.850433Z"
    }
   },
   "cell_type": "code",
   "source": "llm.inspect_history(n=1)",
   "id": "286d0c39728e5956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bootstrapped few-shot\n",
    "Next, we repeat the experiment with the addition of 3 examples bootstrapped by DSPy to optimize the evaluation metrics."
   ],
   "id": "6e0ff9cf578f5b7b"
  },
  {
   "cell_type": "code",
   "source": [
    "for dataset in runners:\n",
    "    runner = runners[dataset]\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for i, j in [[0, 1], [0, 3], [0, 5], [3, 3], [3, 5]]:\n",
    "        print(f\"Few-shot n: {i}, Bootstrapped n: {j}\")\n",
    "        scores = runner.run_bootstrap_few_shot_experiment(n_labeled_few_shot=i, n_bootstrapped_few_shot=j)\n",
    "        update_results(\"bootstrap_few_shot\", dataset, scores, {\"n_few_shot\": i, \"n_bootstrapped_few_shot\": j, \"prompt\": prompts[0]})\n",
    "        pretty_print(scores)\n",
    "        print(\"--\")\n",
    "    print(\"=====\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-30T13:36:25.851436Z"
    }
   },
   "id": "7bb946a284aff621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-30T13:36:25.851436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(\"results.csv\")\n",
    "result_df"
   ],
   "id": "8c9376972f39a1bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
