{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explingo Experiment Runner\n",
    "\n",
    "This notebook:\n",
    "1. Loads the gold-standard dataset, prepares the metrics functions, and verifies that the metric functions give the maximum score on the gold-standard dataset\n",
    "TODO: ensure that narratives very unlike gold-standard score lower\n",
    "2. Runs the prompt-design, few-shot, and bootstrap-few-shot experiments on a testing dataset"
   ],
   "id": "a9c2d95909e76143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Import necessary libraries, prepare the LLM, and load the gold-standard dataset\n",
    "\n",
    "**Note: To run these cells, you need a `keys.yaml` file in the top-level Explingo directory with the following line:**\n",
    "```yaml\n",
    "openai_api_key: <your_openai_api_key>\n",
    "```"
   ],
   "id": "743e31b8c011c24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-20T20:08:25.909348Z",
     "start_time": "2024-08-20T20:08:24.636731Z"
    }
   },
   "source": [
    "import examples, core \n",
    "import os\n",
    "import yaml\n",
    "import dspy\n",
    "import metrics\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "with open(os.path.join(\"..\", \"keys.yaml\"), \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "\n",
    "llm = dspy.OpenAI(model='gpt-4o', api_key=openai_api_key, max_tokens=2000)\n",
    "\n",
    "data = examples.load_examples(\"examples.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T20:08:25.925181Z",
     "start_time": "2024-08-20T20:08:25.910319Z"
    }
   },
   "id": "6e9d56b95001b3ac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some examples include gold-standard narratives; others include only a sample explanation.\n",
    "- The former makes up the gold-standard dataset used for tuning the evaluation metrics and providing few-shot examples.\n",
    "- The latter makes up the testing dataset used for evaluation and for bootstrapping few-shot examples\n",
    "\n",
    "TODO: Right now, we few-shot prompt and evaluation using the same dataset which may lead to biased results, we should separate into a test and train dataset"
   ],
   "id": "70871e15ba810c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T20:08:25.940846Z",
     "start_time": "2024-08-20T20:08:25.926164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gold_standards = [d for d in data if hasattr(d, \"narrative\")]\n",
    "max_optimal_length = max([len(d.narrative) for d in gold_standards])\n",
    "max_optimal_length"
   ],
   "id": "66031abf3885cb9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we set up the evaluation metrics. We use the following metrics, all scored on a scale from 0-2:\n",
    "- Accuracy: the narrative accurately describes the information in the explanation\n",
    "- Fluency: the narrative is coherent and natural, as compared to the gold-standard explanations. We pass in a small list of sample narratives from the gold-standard dataset to compare against\n",
    "- Conciseness: the narrative is not too long, as compared to the gold-standard explanations. For now, any narrative that is no longer than the longest gold-standard narrative will score 2\n",
    "- Context Awareness: the rationalization given alongside the explanation is relevant\n",
    "- Completeness: the narrative includes all relevant information from the original explanation \n",
    "\n",
    "TODO: Currently, we have removed the completeness metric as the accuracy metric ends up encompassing it (and the definition of complete depends on the gold-standard). We need to decide if we want to add it back in\n",
    "\n",
    "**Note: You can set `verbose=1` to see the narratives generated, or `verbose=2` to see the explanations, narratives, and rationalizations**"
   ],
   "id": "733553c62a1d557d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T20:08:25.956910Z",
     "start_time": "2024-08-20T20:08:25.942847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_good_narratives = random.sample([d.narrative for d in gold_standards], 5)\n",
    "example_bad_narratives = random.sample([d.bad_narrative for d in gold_standards], 5)\n",
    "\n",
    "exp_metrics = metrics.Metrics(\n",
    "            [\n",
    "                metrics.accuracy,\n",
    "                metrics.fluency,\n",
    "                metrics.conciseness,\n",
    "                metrics.context_awareness,\n",
    "            ], verbose=0, \n",
    "        metric_kwargs={\"conciseness\": {\"max_optimal_length\": max_optimal_length},\n",
    "                       \"fluency\": {\"good_narratives\": example_good_narratives, \"bad_narratives\": example_bad_narratives}}\n",
    "        )"
   ],
   "id": "75a9fcd3ad0e9975",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we set up the main experiment runner object",
   "id": "aa44b11781eaddb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T20:08:25.972963Z",
     "start_time": "2024-08-20T20:08:25.957909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "explingo = core.Explingo(llm=llm, context=\"The model predicts house prices\", \n",
    "                         examples=data, metric=exp_metrics)"
   ],
   "id": "9ac27ac4e6ca2496",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Verifying the Evaluation Metrics\n",
    "\n",
    "In the following code block, we verify that all the evaluation metrics give the maximum score on the gold-standard dataset. We are defining a successful narrative as one that matches the standards of the gold-standard dataset; this allows us to ensure that the metrics are aligned to this gold-standard.\n",
    "\n",
    "**Note: occasionally, we do see a few failures, but for now most runs I am seeing 100% success.**\n",
    "\n",
    "TODO: To complete this, we also need to ensure that narratives that are unlike the gold-standard score lower, using a separate dataset\n",
    "\n",
    "TODO: We do not currently verify the context awareness metric, as the gold-standard dataset does not include a rationalization"
   ],
   "id": "f370ebdbbdf7fe9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T20:08:25.988608Z",
     "start_time": "2024-08-20T20:08:25.974473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VERIFY = False\n",
    "\n",
    "if VERIFY:\n",
    "    # Gold standard dataset does not include a rationalization, so we skip context awareness \n",
    "    ver_metrics = metrics.Metrics(\n",
    "        [\n",
    "            metrics.accuracy,\n",
    "            metrics.fluency,\n",
    "            metrics.conciseness,\n",
    "        ], verbose=0, \n",
    "        metric_kwargs={\"conciseness\": {\"max_optimal_length\": max_optimal_length},\n",
    "                       \"fluency\": {\"good_narratives\": example_good_narratives, \"bad_narratives\": example_bad_narratives}}\n",
    "    )\n",
    "    total_passed, total_failed = 0, 0\n",
    "    for gold_standard in gold_standards:\n",
    "        result = ver_metrics(gold_standard, gold_standard)[0]\n",
    "        if result != 2*len(ver_metrics.metric_funcs):\n",
    "            print(f\"Failed {gold_standard}\")\n",
    "            total_failed += 1\n",
    "        else:\n",
    "            total_passed += 1\n",
    "    print(f\"Total passed: {total_passed}, total failed: {total_failed}\")"
   ],
   "id": "4b38609ff7969130",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BASIC PROMPTING\n",
    "\n",
    "We begin with basic prompts. With 4 metrics (without completeness), each with a score of 0-2, the maximum score is 8. \n",
    "\n",
    "We generate narratives/rationalizations on `max_iters=5` sample explanations, and return the average total score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5694bfd440f310"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T20:08:26.004232Z",
     "start_time": "2024-08-20T20:08:25.989588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utility for cleaner results\n",
    "\n",
    "def pretty_print(result):\n",
    "    s = f\"Total score: {result[0]}\"\n",
    "    s2 = \", \".join([f\"{k}: {v}\" for k, v in result[1].items()])\n",
    "    print(f\"{s} ({s2})\")"
   ],
   "id": "7b3f0abc65532f7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    \"You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\",\n",
    "    \"You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\",\n",
    "    \"You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    pretty_print(explingo.run_basic_prompting_experiment(data, prompt=prompt, max_iters=5))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T20:08:37.655615Z",
     "start_time": "2024-08-20T20:08:26.005753Z"
    }
   },
   "id": "1be61df6a728fa29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 7.6 (accuracy: 2.0, fluency: 1.6, conciseness: 2.0, context_awareness: 2.0)\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 7.6 (accuracy: 2.0, fluency: 1.6, conciseness: 2.0, context_awareness: 2.0)\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 7.2 (accuracy: 2.0, fluency: 1.2, conciseness: 2.0, context_awareness: 2.0)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we repeat the experiment with the addition of N few-shot examples from the gold-standard dataset.",
   "id": "89d7ba7e0085534b"
  },
  {
   "cell_type": "code",
   "source": [
    "for i in [1, 3, 5]:\n",
    "    print(f\"Few-shot n: {i}\")\n",
    "    pretty_print(explingo.run_few_shot_experiment(data, max_iters=5, n_few_shot=i))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T20:10:00.560125Z",
     "start_time": "2024-08-20T20:08:37.657572Z"
    }
   },
   "id": "c57212f361799ac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot n: 1\n",
      "Total score: 8.0 (accuracy: 2.0, fluency: 2.0, conciseness: 2.0, context_awareness: 2.0)\n",
      "Few-shot n: 3\n",
      "Total score: 8.0 (accuracy: 2.0, fluency: 2.0, conciseness: 2.0, context_awareness: 2.0)\n",
      "Few-shot n: 5\n",
      "Total score: 8.0 (accuracy: 2.0, fluency: 2.0, conciseness: 2.0, context_awareness: 2.0)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we repeat the experiment with the addition of 3 examples bootstrapped by DSPy to optimize the evaluation metrics.\n",
    "\n",
    "TODO: We should experiment with different numbers of labeled few-shot and bootstrapped few-shot examples"
   ],
   "id": "6e0ff9cf578f5b7b"
  },
  {
   "cell_type": "code",
   "source": [
    "for i, j in [[0, 3], [0, 5], [3, 3], [3, 5]]:\n",
    "    print(f\"Few-shot n: {i}, Bootstrapped n: {j}\")\n",
    "    pretty_print(explingo.run_bootstrap_few_shot_experiment(data, max_iters=5, n_labeled_few_shot=3, n_bootstrapped_few_shot=3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T20:12:26.390631Z",
     "start_time": "2024-08-20T20:10:00.561602Z"
    }
   },
   "id": "7bb946a284aff621",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot n: 0, Bootstrapped n: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:06<00:20,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:05<00:17,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:01<00:06,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:05<00:13,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:02<00:11,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 6 examples in round 0.\n",
      "Total score: 7.4 (accuracy: 2.0, fluency: 1.4, conciseness: 2.0, context_awareness: 2.0)\n",
      "Few-shot n: 0, Bootstrapped n: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [00:05<00:12,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [00:06<00:14,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:04<00:15,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:01<00:07,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:02<00:08,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n",
      "Total score: 7.4 (accuracy: 2.0, fluency: 1.4, conciseness: 2.0, context_awareness: 2.0)\n",
      "Few-shot n: 3, Bootstrapped n: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:04<00:10,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:02<00:09,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:03<00:09,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:02<00:06,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:02<00:09,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 6 examples in round 0.\n",
      "Total score: 7.2 (accuracy: 1.8, fluency: 1.4, conciseness: 2.0, context_awareness: 2.0)\n",
      "Few-shot n: 3, Bootstrapped n: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:02<00:07,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:01<00:03,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:01<00:04,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:05<00:16,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:00<00:01, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n",
      "Total score: 7.4 (accuracy: 2.0, fluency: 1.4, conciseness: 2.0, context_awareness: 2.0)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T20:12:26.406655Z",
     "start_time": "2024-08-20T20:12:26.391614Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8b40fd66b580d194",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
