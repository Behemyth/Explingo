{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "md# Explingo Experiment Runner\n",
    "\n",
    "This notebook:\n",
    "1. Loads the gold-standard dataset, prepares the metrics functions, and verifies that the metric functions give the maximum score on the gold-standard dataset, and lower scores on less aligned datasets\n",
    "2. Runs the prompt-design, few-shot, and bootstrap-few-shot experiments on a testing dataset"
   ],
   "id": "a9c2d95909e76143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Import necessary libraries, prepare the LLM, and load the gold-standard dataset\n",
    "\n",
    "**Note: To run these cells, you need a `keys.yaml` file in the top-level Explingo directory with the following line:**\n",
    "```yaml\n",
    "openai_api_key: <your_openai_api_key>\n",
    "```"
   ],
   "id": "743e31b8c011c24"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-27T17:44:32.041886Z",
     "start_time": "2024-08-27T17:44:32.029270Z"
    }
   },
   "source": [
    "import examples, core \n",
    "import os\n",
    "import yaml\n",
    "import dspy\n",
    "import metrics\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "with open(os.path.join(\"..\", \"keys.yaml\"), \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    openai_api_key = config[\"openai_api_key\"]\n",
    "\n",
    "llm = dspy.OpenAI(model='gpt-4o', api_key=openai_api_key, max_tokens=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T17:44:32.088026Z",
     "start_time": "2024-08-27T17:44:32.074884Z"
    }
   },
   "id": "6e9d56b95001b3ac",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Some examples include gold-standard narratives; others include only a sample explanation.\n",
    "- The former makes up the gold-standard dataset used for tuning the evaluation metrics and providing few-shot examples.\n",
    "- The latter makes up the testing dataset used for evaluation and for bootstrapping few-shot examples"
   ],
   "id": "70871e15ba810c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T17:44:32.103884Z",
     "start_time": "2024-08-27T17:44:32.089898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labeled_train, labeled_eval, unlabeled_train, unlabeled_eval = examples.get_data(os.path.join(\"eval_data\", \"gold_standards.json\"))\n",
    "train_data = labeled_train + unlabeled_train\n",
    "eval_data = labeled_eval + unlabeled_eval\n",
    "\n",
    "max_optimal_length = max([len(d.narrative) for d in labeled_train])\n",
    "max_optimal_length"
   ],
   "id": "66031abf3885cb9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we set up the evaluation metrics. We use the following metrics, all scored on a scale from 0-2:\n",
    "- Accuracy: the narrative accurately describes the information in the explanation\n",
    "- Fluency: the narrative is coherent and natural, as compared to the gold-standard explanations. We pass in a small list of sample narratives from the gold-standard dataset to compare against\n",
    "- Conciseness: the narrative is not too long, as compared to the gold-standard explanations. For now, any narrative that is no longer than the longest gold-standard narrative will score 2\n",
    "- Completeness: the narrative includes all relevant information from the original explanation \n",
    "\n",
    "**Note: You can set `verbose=1` to see the narratives generated, or `verbose=2` to see the explanations, narratives, and rationalizations**"
   ],
   "id": "733553c62a1d557d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T17:44:32.119010Z",
     "start_time": "2024-08-27T17:44:32.105885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_good_narratives = random.sample([d.narrative for d in labeled_train], 5)\n",
    "\n",
    "exp_metrics = metrics.Metrics(\n",
    "            [\n",
    "                metrics.accuracy,\n",
    "                metrics.fluency,\n",
    "                metrics.conciseness,\n",
    "                metrics.completeness,\n",
    "            ], verbose=0, openai_key=openai_api_key,\n",
    "        metric_kwargs={\"conciseness\": {\"max_optimal_length\": max_optimal_length},\n",
    "                       \"fluency\": {\"good_narratives\": example_good_narratives}}\n",
    "        )"
   ],
   "id": "75a9fcd3ad0e9975",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, we set up the main experiment runner object"
   ],
   "id": "aa44b11781eaddb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T17:44:32.133890Z",
     "start_time": "2024-08-27T17:44:32.119885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "explingo = core.Explingo(llm=llm, context=\"The model predicts house prices\",\n",
    "                         labeled_train_data=labeled_train, unlabeled_train_data=unlabeled_train)"
   ],
   "id": "9ac27ac4e6ca2496",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic prompt design experiment\n",
    "\n",
    "We begin with basic prompts. With 4 metrics (without completeness), each with a score of 0-2, the maximum score is 8. \n",
    "\n",
    "We generate narratives/rationalizations on `max_iters=5` sample explanations, and return the average total score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5694bfd440f310"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T17:44:32.149321Z",
     "start_time": "2024-08-27T17:44:32.135889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utility for cleaner results\n",
    "\n",
    "def pretty_print(result):\n",
    "    s = f\"Total score: {result[0]}\"\n",
    "    s2 = \", \".join([f\"{k}: {v}\" for k, v in result[1].items()])\n",
    "    print(f\"{s} ({s2})\")"
   ],
   "id": "7b3f0abc65532f7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    \"You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\",\n",
    "    \"You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\",\n",
    "    \"You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    pretty_print(explingo.run_basic_prompting_experiment(eval_data, metrics=exp_metrics, prompt=prompt, max_iters=5))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T17:44:43.257336Z",
     "start_time": "2024-08-27T17:44:32.150317Z"
    }
   },
   "id": "1be61df6a728fa29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\n",
      "Total score: 15.2 (accuracy: 4.0, fluency: 3.6, conciseness: 4.0, completeness: 3.6)\n",
      "Prompt: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\n",
      "Total score: 15.4 (accuracy: 4.0, fluency: 3.4, conciseness: 4.0, completeness: 4.0)\n",
      "Prompt: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\n",
      "Total score: 15.6 (accuracy: 4.0, fluency: 3.6, conciseness: 4.0, completeness: 4.0)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few-shot experiment\n",
    "\n",
    "Next, we repeat the experiment with the addition of N few-shot examples from the gold-standard dataset."
   ],
   "id": "89d7ba7e0085534b"
  },
  {
   "cell_type": "code",
   "source": [
    "for i in [1, 3, 5]:\n",
    "    print(f\"Few-shot n: {i}\")\n",
    "    pretty_print(explingo.run_few_shot_experiment(eval_data, metrics=exp_metrics, max_iters=5, n_few_shot=i))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-27T17:44:43.259351Z"
    }
   },
   "id": "c57212f361799ac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot n: 1\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bootstrapped few-shot\n",
    "Next, we repeat the experiment with the addition of 3 examples bootstrapped by DSPy to optimize the evaluation metrics."
   ],
   "id": "6e0ff9cf578f5b7b"
  },
  {
   "cell_type": "code",
   "source": [
    "for i, j in [[0, 3], [0, 5], [3, 3], [3, 5]]:\n",
    "    print(f\"Few-shot n: {i}, Bootstrapped n: {j}\")\n",
    "    pretty_print(explingo.run_bootstrap_few_shot_experiment(eval_data, metrics=exp_metrics, max_iters=5, n_labeled_few_shot=i, n_bootstrapped_few_shot=j))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "7bb946a284aff621",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
