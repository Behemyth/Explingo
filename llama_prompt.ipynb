{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prerequisite libraries\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import json\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LlamaModel, LlamaForQuestionAnswering, LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE = False\n",
    "N_FEATURES = 3\n",
    "N_RESPONSES = 3\n",
    "#EXPS = [\"ames_housing_0.csv\", \"ames_housing_1.csv\", \"ames_housing_2.csv\"]\n",
    "#EXPS = [\"mushroom_0.csv\", \"mushroom_1.csv\", \"mushroom_2.csv\"]\n",
    "#EXPS = [\"cell_phone_churn_0.csv\", \"cell_phone_churn_1.csv\", \"cell_phone_churn_2.csv\"]\n",
    "EXPS = [\"ames_housing_0.csv\", ]\n",
    "JSON_NAME = \"sample.json\"\n",
    "\n",
    "exps = []\n",
    "for exp in EXPS:\n",
    "  exp_df = pd.read_csv(exp)\n",
    "  exp_df = exp_df.sort_values(by=\"Contribution\", key=abs, ascending=False)\n",
    "  exps.append(exp_df.to_dict('records'))\n",
    "\n",
    "try:\n",
    "  with open(JSON_NAME, \"r\") as fp:\n",
    "      save_json = json.load(fp)\n",
    "except FileNotFoundError:\n",
    "  save_json = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = pd.read_csv(\"ames_housing_0.csv\")\n",
    "exp_df = exp_df.sort_values(by=\"Contribution\", key=abs, ascending=False)\n",
    "exp = exp_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6d4579074045de9688491444d731b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = \"./llama-2-13b-chat-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "tokenizer_llama = LlamaTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_exp(exp, num_features=5, include_average=True):\n",
    "  features = []\n",
    "  if num_features is None:\n",
    "    num_features = len(exp)\n",
    "  for i in range(num_features):\n",
    "    if include_average:\n",
    "      features.append(\"({}, {}, {}, {})\".format(exp[i]['Feature Name'].strip(),\n",
    "                                                exp[i]['Feature Value'],\n",
    "                                                exp[i]['Contribution'],\n",
    "                                                exp[i]['Average/Mode']))\n",
    "    else:\n",
    "      features.append(\"({}, {}, {})\".format(exp[i]['Feature Name'].strip(),\n",
    "                                                exp[i]['Feature Value'].strip(),\n",
    "                                                exp[i]['Contribution']))\n",
    "  return \", \".join(features)\n",
    "\n",
    "def show_responses(response, filename=None):\n",
    "  f = None\n",
    "  if filename is not None:\n",
    "    f = open(filename, \"w\")\n",
    "  for choice in response.choices:\n",
    "    if f is not None:\n",
    "      f.write(choice.message.content)\n",
    "      f.write(\"\\n\")\n",
    "    print(choice.message.content)\n",
    "    print(\"\\n\")\n",
    "  if f is not None:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  (\"You are a helpful assistant. \"\n",
    "           \"You are helping users understand an ML model's predictions. \"\n",
    "           \"Do not use more tokens that necessary but make your answers sound natural.\"\n",
    "           )\n",
    "question =  (\"Convert this feature contibution explanation, generated using SHAP, into a simple narrative. \"\n",
    "             \"The explanation is presented in (feature, feature_value, contribution, average_feature_value) format: \")\n",
    "explanation = parse_exp(exp, num_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_llama = transformers.pipeline(\n",
    "\"text-generation\",\n",
    "model=model,\n",
    "tokenizer=tokenizer_llama)\n",
    "# torch_dtype=torch.float16,\n",
    "# device_map=\"auto\",\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_llama(question, explanation, pipeline=pipeline_llama, tokenizer=tokenizer_llama, json=None):\n",
    "\n",
    "    input = f\"\"\"\n",
    "        <<SYS>>\n",
    "        Only respond with \"Not in the text.\" if the information needed to answer the question is not contained in the document. \\n\n",
    "        Answer the question using only the information from the attached document below. \\n\n",
    "        Ensure that the questions are answered fully and effectively. \\n\n",
    "        Respond in short and concise yet fully formulated sentences, being precise and accurate\n",
    "        {prompt}\n",
    "        <</SYS>>\n",
    "        [INST]\n",
    "        User:{question}\n",
    "        [/INST]\\\n",
    "        [INST]\n",
    "        User:{explanation}\n",
    "        [/INST]\\n\n",
    "\n",
    "        Assistant:\n",
    "    \"\"\"\n",
    "    \n",
    "    sequences = pipeline(\n",
    "        input,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        num_return_sequences=3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=500,\n",
    "        return_full_text=False,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    for seq in sequences:\n",
    "      generated_text = seq['generated_text']\n",
    "      # Find the start of the assistant's answer and return only that part\n",
    "      answer_start = generated_text.find(\"Assistant:\") + len(\"Assistant:\")\n",
    "      response = generated_text[answer_start:].strip()\n",
    "    \n",
    "      if json is not None:\n",
    "          if prompt not in json:\n",
    "              json[prompt] = {}\n",
    "          json[prompt][question] = response\n",
    "          #pp_result(json, prompt, question)\n",
    "      else:\n",
    "          return response\n",
    "\n",
    "def get_responses(response):\n",
    "  responses = []\n",
    "  for choice in response.choices:\n",
    "    responses.append(choice.message.content)\n",
    "  return responses\n",
    "  \n",
    "def pp_result(json, prompt, question):\n",
    "  print(\"PROMPT: %s\" % prompt)\n",
    "  print(\"QUESTION: %s\" % question)\n",
    "  print(\"===\")\n",
    "  for response in json[prompt][question]:\n",
    "    print(response)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Example usage\n",
    "results = generate_answer_llama(question, explanation, json=save_json)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_NAME, \"a\") as fp:\n",
    "  json.dump(save_json, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
